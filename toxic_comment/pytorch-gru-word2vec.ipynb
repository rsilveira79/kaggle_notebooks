{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## Sklearn imports\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "## NLP Libraries\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import download\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "spacy_en = spacy.load('en')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', delimiter=\",\")\n",
    "print(\"Train size: {}\".format(len(train)))\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', delimiter=\",\")\n",
    "print(\"Test size: {}\".format(len(test)))\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, do_stop=False):\n",
    "    text = str(text)\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)  # Strip all the numerics\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) # Removing non ASCII chars\n",
    "    text = text.replace(\"\\n\",\"\") # Removing line breaks\n",
    "    text = text.replace(\"=\",\"\") # Removing =\n",
    "    text = text.replace(\":\",\"\") # Removing :\n",
    "    text = text.replace(\"#\",\"\") # Removing #\n",
    "    text = text.replace(\"%\",\"\") # Removing #\n",
    "    text = text.replace(\"&\",\"\") # Removing #\n",
    "    text = text.replace('\"',\"\") # Removing #\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)# Strip multiple whitespaces\n",
    "\n",
    "    text = text.lower()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stops]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "    text = \" \".join(filtered_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_comment']=train['comment_text'].apply(lambda x:clean_text(x, do_stop=True))\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['cleaned_comment']=test['comment_text'].apply(lambda x:clean_text(x, do_stop=True))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train['cleaned_comment'],\n",
    "                                                    list(zip(train['toxic'], train['severe_toxic'],\n",
    "                                                             train['obscene'], train['threat'],\n",
    "                                                             train['insult'], train['identity_hate'])),\n",
    "                                                      test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test['cleaned_comment'])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=list(zip(x_train,y_train))\n",
    "train_data[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data=list(zip(x_valid,y_valid))\n",
    "valid_data[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Vocabulary\n",
    "word_to_ix = {}\n",
    "for (sent) in list(x_train) + list(x_valid)+list(x_test):\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 6\n",
    "VOCAB_SIZE,NUM_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../vectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('../../vectors/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM=300\n",
    "sd = 1/np.sqrt(W2V_DIM) ## standard deviation to use\n",
    "weights = np.random.normal(0, scale=sd, size=[VOCAB_SIZE, W2V_DIM])\n",
    "weights = weights.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_to_ix:\n",
    "    id = word_to_ix.get(word,None)\n",
    "    if id is not None:\n",
    "        try:\n",
    "            weights[id]=w2v.wv.word_vec(word)\n",
    "        except:\n",
    "            weights[id]=np.random.normal(0, scale=sd, size=[1, W2V_DIM]) ## If word not present, initialize randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix['sky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.word_vec(\"delete\")[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=word_to_ix['delete']\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[word][0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruClassifierW2vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, label_size, pre_trained_weights, dropout):\n",
    "        super(GruClassifierW2vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #print(\"Pre-trained weights for word [delete]: \\n{}\".format(pre_trained_weights[word_2_idx['delete']][0:50]))\n",
    "        self.word_embeddings.weight.data=torch.Tensor(pre_trained_weights)\n",
    "        self.gru = nn.GRU(input_size = embedding_dim,\n",
    "                            hidden_size = hidden_dim,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        return (Variable(torch.zeros(self.num_layers, 1, self.hidden_dim))).cuda()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        for i in range(self.num_layers):\n",
    "            gru_out, self.hidden = self.gru(x, self.hidden)\n",
    "        y  = self.hidden2label(gru_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq.split()]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label):\n",
    "    return torch.FloatTensor(label).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM = 300\n",
    "HIDDEN_DIM = 80\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GruClassifierW2vec(embedding_dim=W2V_DIM,\n",
    "                            hidden_dim=HIDDEN_DIM,\n",
    "                            num_layers=NUM_LAYERS,\n",
    "                            vocab_size=VOCAB_SIZE,\n",
    "                            label_size=NUM_LABELS,\n",
    "                            pre_trained_weights = weights,\n",
    "                            dropout = DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=train_data[0][0]\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_1=Variable(make_context_vector(msg,word_to_ix)).cuda()\n",
    "samp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(samp_1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=train_data[2][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context=Variable(make_context_vector(sample,word_to_ix)).cuda()\n",
    "sample_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(sample_context)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (sent,label) in train_data[0:2]:\n",
    "    print(sent)\n",
    "    print(label)\n",
    "    print(Variable(make_target(label)))\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_iters = 2000\n",
    "num_epochs = n_iters/(len(x_train))/batch_size\n",
    "num_epochs=int(num_epochs)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for (sent,label) in train_data:\n",
    "        # Step 1 - clear the gradients\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        ## Step 2- Prepare input and label\n",
    "        context_vec = Variable(make_context_vector(sent, word_to_ix)).cuda()\n",
    "        target = Variable(make_target(label)).cuda()\n",
    "        \n",
    "        # Step 3 - Run forward pass\n",
    "        output = model(context_vec)\n",
    "       \n",
    "        # Step 4 - Compute loss, gradients, update parameters\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter+=1      \n",
    "        ## Calculate final accuracy\n",
    "        if iter % 500 ==0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (sent,label) in valid_data:\n",
    "                context_vec = Variable(make_context_vector(sent, word_to_ix)).cuda()\n",
    "                target = Variable(make_target(label)).cuda()\n",
    "                output = model(context_vec)\n",
    "                _,predicted = torch.max(output.data,1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted[0] == make_target(label)).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print('Iterations: {}. Loss: {}. Accuracy: {}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
