{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "## Torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## Sklearn imports\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "## NLP Libraries\n",
    "# Spacy\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "# NLTK\n",
    "from nltk import download\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.5\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 95851\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95846</th>\n",
       "      <td>999977655955</td>\n",
       "      <td>\"\\nI have discussed it, unlike most of those w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95847</th>\n",
       "      <td>999982426659</td>\n",
       "      <td>ps. Almost forgot, Paine don't reply back to t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95848</th>\n",
       "      <td>999982764066</td>\n",
       "      <td>Mamoun Darkazanli\\nFor some reason I am unable...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95849</th>\n",
       "      <td>999986890563</td>\n",
       "      <td>Salafi would be a better term. It is more poli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95850</th>\n",
       "      <td>999988164717</td>\n",
       "      <td>making wikipedia a better and more inviting pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "95846  999977655955  \"\\nI have discussed it, unlike most of those w...      0   \n",
       "95847  999982426659  ps. Almost forgot, Paine don't reply back to t...      1   \n",
       "95848  999982764066  Mamoun Darkazanli\\nFor some reason I am unable...      0   \n",
       "95849  999986890563  Salafi would be a better term. It is more poli...      0   \n",
       "95850  999988164717  making wikipedia a better and more inviting pl...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \n",
       "95846             0        0       0       0              0  \n",
       "95847             0        1       0       0              0  \n",
       "95848             0        0       0       0              0  \n",
       "95849             0        0       0       0              0  \n",
       "95850             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', delimiter=\",\")\n",
    "print(\"Train size: {}\".format(len(train)))\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 226998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226993</th>\n",
       "      <td>999966872214</td>\n",
       "      <td>*{Persondata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226994</th>\n",
       "      <td>999968525410</td>\n",
       "      <td>'' —  is wishing you a [WIKI_LINK: Mary Poppin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226995</th>\n",
       "      <td>999980053494</td>\n",
       "      <td>==Fair use rationale for [WIKI_LINK: Image:D.R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226996</th>\n",
       "      <td>999980680364</td>\n",
       "      <td>== Employment Practices at Majestic ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226997</th>\n",
       "      <td>999997819802</td>\n",
       "      <td>Welcome to Wikipedia. Although everyone is wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text\n",
       "226993  999966872214                                       *{Persondata\n",
       "226994  999968525410  '' —  is wishing you a [WIKI_LINK: Mary Poppin...\n",
       "226995  999980053494  ==Fair use rationale for [WIKI_LINK: Image:D.R...\n",
       "226996  999980680364             == Employment Practices at Majestic ==\n",
       "226997  999997819802  Welcome to Wikipedia. Although everyone is wel..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv', delimiter=\",\")\n",
    "print(\"Test size: {}\".format(len(test)))\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer_spacy(text):        \n",
    "    sent = []\n",
    "    doc = spacy_en(text)\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        if word.lemma_ == \"-PRON-\":\n",
    "            sent.append(word.text)\n",
    "        else:\n",
    "            sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    return ''.join(c for c in text if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, do_stop=False, do_lemma= False):\n",
    "    text = str(text)\n",
    "    #text = gensim.parsing.preprocessing.strip_numeric(text)  # Strip all the numerics\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) # Removing non ASCII chars\n",
    "    text = text.replace(\"\\n\",\"\") # Removing line breaks\n",
    "\n",
    "    # Remove the punctuation\n",
    "    text = strip_punctuation(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stops]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    ## Lemmatization\n",
    "    if (do_lemma==True):\n",
    "    #    text = lemmatizer_spacy(text)\n",
    "        text = lemmatizer.lemmatize(text) ## using NLTK lemmatizer\n",
    "        \n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)# Strip multiple whitespaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that is not cool'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"\\n ##?? %&that is not cool\"\n",
    "clean_text(msg, do_lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95846</th>\n",
       "      <td>999977655955</td>\n",
       "      <td>\"\\nI have discussed it, unlike most of those w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i have discussed it unlike most of those who r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95847</th>\n",
       "      <td>999982426659</td>\n",
       "      <td>ps. Almost forgot, Paine don't reply back to t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ps almost forgot paine dont reply back to this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95848</th>\n",
       "      <td>999982764066</td>\n",
       "      <td>Mamoun Darkazanli\\nFor some reason I am unable...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mamoun darkazanlifor some reason i am unable t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95849</th>\n",
       "      <td>999986890563</td>\n",
       "      <td>Salafi would be a better term. It is more poli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>salafi would be a better term it is more polit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95850</th>\n",
       "      <td>999988164717</td>\n",
       "      <td>making wikipedia a better and more inviting pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>making wikipedia a better and more inviting place</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "95846  999977655955  \"\\nI have discussed it, unlike most of those w...      0   \n",
       "95847  999982426659  ps. Almost forgot, Paine don't reply back to t...      1   \n",
       "95848  999982764066  Mamoun Darkazanli\\nFor some reason I am unable...      0   \n",
       "95849  999986890563  Salafi would be a better term. It is more poli...      0   \n",
       "95850  999988164717  making wikipedia a better and more inviting pl...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "95846             0        0       0       0              0   \n",
       "95847             0        1       0       0              0   \n",
       "95848             0        0       0       0              0   \n",
       "95849             0        0       0       0              0   \n",
       "95850             0        0       0       0              0   \n",
       "\n",
       "                                         cleaned_comment  \n",
       "95846  i have discussed it unlike most of those who r...  \n",
       "95847  ps almost forgot paine dont reply back to this...  \n",
       "95848  mamoun darkazanlifor some reason i am unable t...  \n",
       "95849  salafi would be a better term it is more polit...  \n",
       "95850  making wikipedia a better and more inviting place  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_comment']=train['comment_text'].apply(lambda x:clean_text(x, do_lemma = True))\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "      <td>orphaned nonfree media image41cd1jboevl ss500 jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "      <td>kentuckiana is colloquial even though the area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "      <td>hello fellow wikipediansi have just modified o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "      <td>akc suspensions the morning call feb 24 2001 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "      <td>wikilink talkcelts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  \\\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...   \n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...   \n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...   \n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...   \n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] ==   \n",
       "\n",
       "                                     cleaned_comment  \n",
       "0  orphaned nonfree media image41cd1jboevl ss500 jpg  \n",
       "1  kentuckiana is colloquial even though the area...  \n",
       "2  hello fellow wikipediansi have just modified o...  \n",
       "3  akc suspensions the morning call feb 24 2001 7...  \n",
       "4                                 wikilink talkcelts  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['cleaned_comment']=test['comment_text'].apply(lambda x:clean_text(x, do_lemma=True))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(zip(train['toxic'], \n",
    "                    train['severe_toxic'],\n",
    "                    train['obscene'], \n",
    "                    train['threat'],\n",
    "                    train['insult'], \n",
    "                    train['identity_hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train['cleaned_comment'],\n",
    "                                                      labels, \n",
    "                                                      test_size=0.2,random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test['cleaned_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filmfare award for best male debutvenky that editor shshshis not updating the correct years for filmfare award for best male debut please check my addition done on 2325 17 june 2011 which is absolutely true please inform shshto update correct years also please check my talk in shsh page regarding same header',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('charles sumner article hello dr jensen i have recently been making edits on the charles sumner article i have expanded on the dominican republic annexation treaty and information on president grant are there any other areas that need work on the cs article',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('nsa conspiracy theory please contain all discussion of the dubious sourcestatements here',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('noted ill try to dig around a bit more to see if theres anything else that could be used to address this additionally ive added classical to the genre bit as per his choices and flowers release this makes me wonder if new age could also be used which would kind of help fill the search for something that details his music other than the widely applicable hiphop thoughts friend',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('since apparently only one user would be so bold to say something like that about steve its a safe bet thats him especially considering he posted on the talk page of the ip that started this whole mess in the first place',\n",
       "  (0, 0, 0, 0, 0, 0))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=list(zip(x_train,y_train))\n",
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reversing her early casual antisemitism when did this get added and where was it discussed meantime i have taken it out',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('i dont fix disambig by awb but just and it is working well',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('the phrase in europe it is refered to as white spirit was especially helpful for me it let me know that white spirit and mineral spirits are the same',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('fhu editingplease explain to me how my editing of the freedhardeman university page was biased and not neutral preceding unsigned comment added by talk contribs',\n",
       "  (0, 0, 0, 0, 0, 0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data=list(zip(x_valid,y_valid))\n",
    "valid_data[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build vocabulary of words\n",
    "word_to_ix = {}\n",
    "for (sent) in list(x_train) + list(x_valid) + list(x_test):\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filmfare': 0,\n",
       " 'award': 1,\n",
       " 'for': 2,\n",
       " 'best': 3,\n",
       " 'male': 4,\n",
       " 'debutvenky': 5,\n",
       " 'that': 6,\n",
       " 'editor': 7,\n",
       " 'shshshis': 8,\n",
       " 'not': 9,\n",
       " 'updating': 10,\n",
       " 'the': 11,\n",
       " 'correct': 12,\n",
       " 'years': 13,\n",
       " 'debut': 14,\n",
       " 'please': 15,\n",
       " 'check': 16,\n",
       " 'my': 17,\n",
       " 'addition': 18,\n",
       " 'done': 19,\n",
       " 'on': 20,\n",
       " '2325': 21,\n",
       " '17': 22,\n",
       " 'june': 23,\n",
       " '2011': 24,\n",
       " 'which': 25,\n",
       " 'is': 26,\n",
       " 'absolutely': 27,\n",
       " 'true': 28,\n",
       " 'inform': 29,\n",
       " 'shshto': 30,\n",
       " 'update': 31,\n",
       " 'also': 32,\n",
       " 'talk': 33,\n",
       " 'in': 34,\n",
       " 'shsh': 35,\n",
       " 'page': 36,\n",
       " 'regarding': 37,\n",
       " 'same': 38,\n",
       " 'header': 39,\n",
       " 'charles': 40,\n",
       " 'sumner': 41,\n",
       " 'article': 42,\n",
       " 'hello': 43,\n",
       " 'dr': 44,\n",
       " 'jensen': 45,\n",
       " 'i': 46,\n",
       " 'have': 47,\n",
       " 'recently': 48,\n",
       " 'been': 49,\n",
       " 'making': 50,\n",
       " 'edits': 51,\n",
       " 'expanded': 52,\n",
       " 'dominican': 53,\n",
       " 'republic': 54,\n",
       " 'annexation': 55,\n",
       " 'treaty': 56,\n",
       " 'and': 57,\n",
       " 'information': 58,\n",
       " 'president': 59,\n",
       " 'grant': 60,\n",
       " 'are': 61,\n",
       " 'there': 62,\n",
       " 'any': 63,\n",
       " 'other': 64,\n",
       " 'areas': 65,\n",
       " 'need': 66,\n",
       " 'work': 67,\n",
       " 'cs': 68,\n",
       " 'nsa': 69,\n",
       " 'conspiracy': 70,\n",
       " 'theory': 71,\n",
       " 'contain': 72,\n",
       " 'all': 73,\n",
       " 'discussion': 74,\n",
       " 'of': 75,\n",
       " 'dubious': 76,\n",
       " 'sourcestatements': 77,\n",
       " 'here': 78,\n",
       " 'noted': 79,\n",
       " 'ill': 80,\n",
       " 'try': 81,\n",
       " 'to': 82,\n",
       " 'dig': 83,\n",
       " 'around': 84,\n",
       " 'a': 85,\n",
       " 'bit': 86,\n",
       " 'more': 87,\n",
       " 'see': 88,\n",
       " 'if': 89,\n",
       " 'theres': 90,\n",
       " 'anything': 91,\n",
       " 'else': 92,\n",
       " 'could': 93,\n",
       " 'be': 94,\n",
       " 'used': 95,\n",
       " 'address': 96,\n",
       " 'this': 97,\n",
       " 'additionally': 98,\n",
       " 'ive': 99,\n",
       " 'added': 100,\n",
       " 'classical': 101,\n",
       " 'genre': 102,\n",
       " 'as': 103,\n",
       " 'per': 104,\n",
       " 'his': 105,\n",
       " 'choices': 106,\n",
       " 'flowers': 107,\n",
       " 'release': 108,\n",
       " 'makes': 109,\n",
       " 'me': 110,\n",
       " 'wonder': 111,\n",
       " 'new': 112,\n",
       " 'age': 113,\n",
       " 'would': 114,\n",
       " 'kind': 115,\n",
       " 'help': 116,\n",
       " 'fill': 117,\n",
       " 'search': 118,\n",
       " 'something': 119,\n",
       " 'details': 120,\n",
       " 'music': 121,\n",
       " 'than': 122,\n",
       " 'widely': 123,\n",
       " 'applicable': 124,\n",
       " 'hiphop': 125,\n",
       " 'thoughts': 126,\n",
       " 'friend': 127,\n",
       " 'since': 128,\n",
       " 'apparently': 129,\n",
       " 'only': 130,\n",
       " 'one': 131,\n",
       " 'user': 132,\n",
       " 'so': 133,\n",
       " 'bold': 134,\n",
       " 'say': 135,\n",
       " 'like': 136,\n",
       " 'about': 137,\n",
       " 'steve': 138,\n",
       " 'its': 139,\n",
       " 'safe': 140,\n",
       " 'bet': 141,\n",
       " 'thats': 142,\n",
       " 'him': 143,\n",
       " 'especially': 144,\n",
       " 'considering': 145,\n",
       " 'he': 146,\n",
       " 'posted': 147,\n",
       " 'ip': 148,\n",
       " 'started': 149,\n",
       " 'whole': 150,\n",
       " 'mess': 151,\n",
       " 'first': 152,\n",
       " 'place': 153,\n",
       " 'agree': 154,\n",
       " 'nobody': 155,\n",
       " 'has': 156,\n",
       " 'write': 157,\n",
       " 'it': 158,\n",
       " 'should': 159,\n",
       " 'merged': 160,\n",
       " 'back': 161,\n",
       " '2005': 162,\n",
       " 'atlantic': 163,\n",
       " 'hurricane': 164,\n",
       " 'season': 165,\n",
       " 'reason': 166,\n",
       " 'nothing': 167,\n",
       " 'do': 168,\n",
       " 'with': 169,\n",
       " 'notability': 170,\n",
       " 'alpha': 171,\n",
       " 'wasnt': 172,\n",
       " 'way': 173,\n",
       " 'notable': 174,\n",
       " 'but': 175,\n",
       " 'point': 176,\n",
       " 'exist': 177,\n",
       " 'already': 178,\n",
       " 'exact': 179,\n",
       " 'thing': 180,\n",
       " 'summary': 181,\n",
       " 'shorter': 182,\n",
       " 'full': 183,\n",
       " 'text': 184,\n",
       " 'however': 185,\n",
       " 'no': 186,\n",
       " 'an': 187,\n",
       " 'edit': 188,\n",
       " 'war': 189,\n",
       " 'storm': 190,\n",
       " 'longer': 191,\n",
       " 'active': 192,\n",
       " 'decision': 193,\n",
       " 'isnt': 194,\n",
       " 'timesensitive': 195,\n",
       " 'suggestion': 196,\n",
       " 'add': 197,\n",
       " 'give': 198,\n",
       " 'people': 199,\n",
       " 'few': 200,\n",
       " 'weeks': 201,\n",
       " 'nightstallions': 202,\n",
       " 'wife': 203,\n",
       " 'got': 204,\n",
       " 'fucked': 205,\n",
       " 'by': 206,\n",
       " 'nigger': 207,\n",
       " 'had': 208,\n",
       " 'babyand': 209,\n",
       " 'smelled': 210,\n",
       " 'fried': 211,\n",
       " 'chicken': 212,\n",
       " 'calling': 213,\n",
       " 'dickhead': 214,\n",
       " 'personal': 215,\n",
       " 'attack': 216,\n",
       " 'sorry': 217,\n",
       " 'fucks': 218,\n",
       " 'again': 219,\n",
       " 'will': 220,\n",
       " 'call': 221,\n",
       " 'utc': 222,\n",
       " 'unfortunately': 223,\n",
       " 'nonsollog': 224,\n",
       " 'get': 225,\n",
       " 'whacked': 226,\n",
       " '2324': 227,\n",
       " '18': 228,\n",
       " 'jan': 229,\n",
       " 'orphaned': 230,\n",
       " 'fair': 231,\n",
       " 'use': 232,\n",
       " 'image': 233,\n",
       " 'imagebtt': 234,\n",
       " 'uk': 235,\n",
       " '1jpgthanks': 236,\n",
       " 'uploading': 237,\n",
       " '1jpg': 238,\n",
       " 'notice': 239,\n",
       " 'currently': 240,\n",
       " 'specifies': 241,\n",
       " 'unlicensed': 242,\n",
       " 'wikipedia': 243,\n",
       " 'may': 244,\n",
       " 'under': 245,\n",
       " 'claim': 246,\n",
       " 'meaning': 247,\n",
       " 'articles': 248,\n",
       " 'was': 249,\n",
       " 'previously': 250,\n",
       " 'go': 251,\n",
       " 'why': 252,\n",
       " 'removed': 253,\n",
       " 'you': 254,\n",
       " 'think': 255,\n",
       " 'useful': 256,\n",
       " 'note': 257,\n",
       " 'images': 258,\n",
       " 'replacement': 259,\n",
       " 'created': 260,\n",
       " 'acceptable': 261,\n",
       " 'our': 262,\n",
       " 'policyif': 263,\n",
       " 'uploaded': 264,\n",
       " 'media': 265,\n",
       " 'whether': 266,\n",
       " 'theyre': 267,\n",
       " 'or': 268,\n",
       " 'can': 269,\n",
       " 'find': 270,\n",
       " 'list': 271,\n",
       " 'pages': 272,\n",
       " 'edited': 273,\n",
       " 'clicking': 274,\n",
       " 'contributions': 275,\n",
       " 'link': 276,\n",
       " 'located': 277,\n",
       " 'at': 278,\n",
       " 'very': 279,\n",
       " 'top': 280,\n",
       " 'when': 281,\n",
       " 'logged': 282,\n",
       " 'then': 283,\n",
       " 'selecting': 284,\n",
       " 'from': 285,\n",
       " 'dropdown': 286,\n",
       " 'box': 287,\n",
       " 'deleted': 288,\n",
       " 'after': 289,\n",
       " 'seven': 290,\n",
       " 'days': 291,\n",
       " 'described': 292,\n",
       " 'criteria': 293,\n",
       " 'speedy': 294,\n",
       " 'deletion': 295,\n",
       " 'thank': 296,\n",
       " 'contr': 297,\n",
       " 'fully': 298,\n",
       " 'vandalize': 299,\n",
       " 'did': 300,\n",
       " 'mary': 301,\n",
       " 'poppins': 302,\n",
       " 'continue': 303,\n",
       " 'blocked': 304,\n",
       " 'editing': 305,\n",
       " 'gabsadds': 306,\n",
       " 'havent': 307,\n",
       " 'responded': 308,\n",
       " 'recent': 309,\n",
       " 'messages': 310,\n",
       " 'doesnt': 311,\n",
       " 'matter': 312,\n",
       " 'anyway': 313,\n",
       " 'useressjay': 314,\n",
       " '24': 315,\n",
       " 'hours': 316,\n",
       " 'hes': 317,\n",
       " 'allergic': 318,\n",
       " 'phrase': 319,\n",
       " 'reply': 320,\n",
       " 'another': 321,\n",
       " 'admin': 322,\n",
       " 'seem': 323,\n",
       " 'understand': 324,\n",
       " 'exactly': 325,\n",
       " 'what': 326,\n",
       " 'means': 327,\n",
       " 'obviously': 328,\n",
       " 'entirely': 329,\n",
       " 'impressed': 330,\n",
       " 'being': 331,\n",
       " 'some': 332,\n",
       " 'even': 333,\n",
       " 'repeated': 334,\n",
       " 'attacks': 335,\n",
       " 'zyx': 336,\n",
       " 'language': 337,\n",
       " 'survey': 338,\n",
       " 'jericho': 339,\n",
       " 'harris': 340,\n",
       " 'poor': 341,\n",
       " 'examples': 342,\n",
       " 'look': 343,\n",
       " 'kevin': 344,\n",
       " 'nash': 345,\n",
       " 'hulk': 346,\n",
       " 'hogan': 347,\n",
       " 'scott': 348,\n",
       " 'hall': 349,\n",
       " 'jeff': 350,\n",
       " 'jarrett': 351,\n",
       " 'while': 352,\n",
       " 'they': 353,\n",
       " 'left': 354,\n",
       " 'wwfwcw': 355,\n",
       " 'their': 356,\n",
       " 'chronological': 357,\n",
       " 'order': 358,\n",
       " 'didnt': 359,\n",
       " 'anywhere': 360,\n",
       " 'endless': 361,\n",
       " 'dan': 362,\n",
       " 're': 363,\n",
       " 'bio': 364,\n",
       " 'templates': 365,\n",
       " 'thanks': 366,\n",
       " 'comments': 367,\n",
       " 'corrected': 368,\n",
       " 'solomons': 369,\n",
       " 'seychelles': 370,\n",
       " 'comoros': 371,\n",
       " 'deliberatly': 372,\n",
       " 'out': 373,\n",
       " 'macau': 374,\n",
       " 'number': 375,\n",
       " 'countries': 376,\n",
       " 'know': 377,\n",
       " 'past': 378,\n",
       " 'discussions': 379,\n",
       " 'taken': 380,\n",
       " 'country': 381,\n",
       " 'decided': 382,\n",
       " 'wave': 383,\n",
       " 'play': 384,\n",
       " 'member': 385,\n",
       " 'un': 386,\n",
       " 'well': 387,\n",
       " 'mmove': 388,\n",
       " 'others': 389,\n",
       " 'later': 390,\n",
       " 'such': 391,\n",
       " 'gibralter': 392,\n",
       " 'vatican': 393,\n",
       " 'city': 394,\n",
       " 'etc': 395,\n",
       " 'now': 396,\n",
       " 'enough': 397,\n",
       " 'neiln': 398,\n",
       " 'every': 399,\n",
       " 'real': 400,\n",
       " 'ones': 401,\n",
       " 'truth': 402,\n",
       " 'let': 403,\n",
       " 'straight': 404,\n",
       " '4': 405,\n",
       " 'oclock': 406,\n",
       " 'morning': 407,\n",
       " 'hour': 408,\n",
       " 'sane': 409,\n",
       " 'bed': 410,\n",
       " 'revert': 411,\n",
       " 'changes': 412,\n",
       " 'your': 413,\n",
       " 'buddy': 414,\n",
       " 'sergecross': 415,\n",
       " 'threatened': 416,\n",
       " 'badlyspelled': 417,\n",
       " 'screed': 418,\n",
       " 'pointing': 419,\n",
       " 'naughty': 420,\n",
       " 'boy': 421,\n",
       " 'amok': 422,\n",
       " 'whats': 423,\n",
       " 'going': 424,\n",
       " 'wikistalking': 425,\n",
       " 'provocation': 426,\n",
       " 'hope': 427,\n",
       " 'angry': 428,\n",
       " 'names': 429,\n",
       " 'worst': 430,\n",
       " 'crime': 431,\n",
       " 'ever': 432,\n",
       " 'rolls': 433,\n",
       " 'fainting': 434,\n",
       " 'couch': 435,\n",
       " 'proceeds': 436,\n",
       " 'ban': 437,\n",
       " 'most': 438,\n",
       " 'remaining': 439,\n",
       " 'millenium': 440,\n",
       " 'justice': 441,\n",
       " 'much': 442,\n",
       " 'rejoicingtell': 443,\n",
       " 'ya': 444,\n",
       " 'change': 445,\n",
       " 'indeed': 446,\n",
       " 'anybodys': 447,\n",
       " 'into': 448,\n",
       " 'want': 449,\n",
       " 'translate': 450,\n",
       " 'them': 451,\n",
       " 'serbocroatian': 452,\n",
       " 'substitute': 453,\n",
       " 'sam': 454,\n",
       " 'chowderhead': 455,\n",
       " 'george': 456,\n",
       " 'w': 457,\n",
       " 'bush': 458,\n",
       " 'world': 459,\n",
       " 'football': 460,\n",
       " 'league': 461,\n",
       " 'marxist': 462,\n",
       " 'treatise': 463,\n",
       " 'asserts': 464,\n",
       " 'washington': 465,\n",
       " 'dick': 466,\n",
       " 'clark': 467,\n",
       " 'were': 468,\n",
       " 'gay': 469,\n",
       " 'lovers': 470,\n",
       " 'christmas': 471,\n",
       " 'shall': 472,\n",
       " 'charitableyoure': 473,\n",
       " 'welcome': 474,\n",
       " 'youbeen': 475,\n",
       " 'eithera': 476,\n",
       " 'drinking': 477,\n",
       " 'too': 478,\n",
       " 'caffenineb': 479,\n",
       " 'taking': 480,\n",
       " 'drugsc': 481,\n",
       " 'just': 482,\n",
       " 'felt': 483,\n",
       " 'loosing': 484,\n",
       " 'mind': 485,\n",
       " 'purpose': 486,\n",
       " 'tech': 487,\n",
       " 'escape': 488,\n",
       " 'orbit': 489,\n",
       " 'yeah': 490,\n",
       " 'looks': 491,\n",
       " 'am': 492,\n",
       " 'uncivil': 493,\n",
       " 'because': 494,\n",
       " 'getting': 495,\n",
       " 'otherwise': 496,\n",
       " 'simply': 497,\n",
       " 'put': 498,\n",
       " 'moron': 499,\n",
       " 'yes': 500,\n",
       " 'assuming': 501,\n",
       " 'said': 502,\n",
       " 'never': 503,\n",
       " 'how': 504,\n",
       " 'morons': 505,\n",
       " 'hence': 506,\n",
       " 'description': 507,\n",
       " 'tone': 508,\n",
       " 'least': 509,\n",
       " 'combative': 510,\n",
       " 'mine': 511,\n",
       " 'who': 512,\n",
       " 'needs': 513,\n",
       " 'calm': 514,\n",
       " 'down': 515,\n",
       " 'actually': 516,\n",
       " 'youyou': 517,\n",
       " 'dont': 518,\n",
       " 'quite': 519,\n",
       " '3rr': 520,\n",
       " 'works': 521,\n",
       " 'someone': 522,\n",
       " 'reverts': 523,\n",
       " '3': 524,\n",
       " 'times': 525,\n",
       " 'worth': 526,\n",
       " 'time': 527,\n",
       " 'broken': 528,\n",
       " 'until': 529,\n",
       " 'thenyou': 530,\n",
       " 'guys': 531,\n",
       " 'team': 532,\n",
       " 'consensus': 533,\n",
       " 'despite': 534,\n",
       " 'im': 535,\n",
       " 'part': 536,\n",
       " 'editors': 537,\n",
       " 'dispute': 538,\n",
       " 'still': 539,\n",
       " 'answered': 540,\n",
       " 'supposed': 541,\n",
       " 'gone': 542,\n",
       " 'deeper': 543,\n",
       " 'satisfied': 544,\n",
       " 'before': 545,\n",
       " 'perhaps': 546,\n",
       " 'ask': 547,\n",
       " 'doing': 548,\n",
       " 'fakesmile': 549,\n",
       " 'fact': 550,\n",
       " 'she': 551,\n",
       " 'editwarring': 552,\n",
       " 'shes': 553,\n",
       " 'mean': 554,\n",
       " 'issue': 555,\n",
       " 'toookay': 556,\n",
       " 'reversion': 557,\n",
       " 'junk': 558,\n",
       " 'where': 559,\n",
       " 'stylized': 560,\n",
       " 'ebay': 561,\n",
       " 'without': 562,\n",
       " 'old': 563,\n",
       " 'version': 564,\n",
       " 'up': 565,\n",
       " 'bullets': 566,\n",
       " 'reversions': 567,\n",
       " 'those': 568,\n",
       " 'early': 569,\n",
       " 'little': 570,\n",
       " 'group': 571,\n",
       " 'called': 572,\n",
       " 'originally': 573,\n",
       " 'reverting': 574,\n",
       " 'leaving': 575,\n",
       " 'mention': 576,\n",
       " 'current': 577,\n",
       " 'style': 578,\n",
       " 'lead': 579,\n",
       " 'according': 580,\n",
       " 'earlier': 581,\n",
       " 'saying': 582,\n",
       " 'mentioning': 583,\n",
       " 'former': 584,\n",
       " 'erase': 585,\n",
       " 'both': 586,\n",
       " 'erasing': 587,\n",
       " 'though': 588,\n",
       " 'original': 589,\n",
       " 'concern': 590,\n",
       " 'against': 591,\n",
       " 'one7516221181': 592,\n",
       " 'april': 593,\n",
       " '2008': 594,\n",
       " 'utcok': 595,\n",
       " 'heres': 596,\n",
       " 'really': 597,\n",
       " 'problem': 598,\n",
       " 'u': 599,\n",
       " 'ppl': 600,\n",
       " 'shit': 601,\n",
       " 'theirs': 602,\n",
       " 'metallica': 603,\n",
       " 'pissed': 604,\n",
       " 'off': 605,\n",
       " 'we': 606,\n",
       " 'breaking': 607,\n",
       " 'points': 608,\n",
       " 'happens': 609,\n",
       " 'weaker': 610,\n",
       " 'urs': 611,\n",
       " 'admit': 612,\n",
       " 'metal': 613,\n",
       " 'abot': 614,\n",
       " 'alot': 615,\n",
       " 'happy': 616,\n",
       " 'area': 617,\n",
       " 'went': 618,\n",
       " 'far': 619,\n",
       " 'mile': 620,\n",
       " 'suggesting': 621,\n",
       " 'end': 622,\n",
       " 'insult': 623,\n",
       " 'twice': 624,\n",
       " 'bad': 625,\n",
       " 'untill': 626,\n",
       " 'words': 627,\n",
       " 'rest': 628,\n",
       " 'wont': 629,\n",
       " 'prove': 630,\n",
       " 'each': 631,\n",
       " 'life': 632,\n",
       " 'problems': 633,\n",
       " 'bother': 634,\n",
       " 'wrong': 635,\n",
       " 'abit': 636,\n",
       " 'defence': 637,\n",
       " 'deserve': 638,\n",
       " 'iv': 639,\n",
       " 'deserved': 640,\n",
       " 'body': 641,\n",
       " 'kicked': 642,\n",
       " 'arse': 643,\n",
       " 'gonna': 644,\n",
       " 'happen': 645,\n",
       " 'soon': 646,\n",
       " 'stupid': 647,\n",
       " 'stop': 648,\n",
       " 'good': 649,\n",
       " 'whatever': 650,\n",
       " 'stoping': 651,\n",
       " 'won': 652,\n",
       " 'win': 653,\n",
       " 'cheers': 654,\n",
       " '0044': 655,\n",
       " '15': 656,\n",
       " '13': 657,\n",
       " 'november': 658,\n",
       " '2009': 659,\n",
       " 'utcwhat': 660,\n",
       " 'jacob': 661,\n",
       " 'peters': 662,\n",
       " 'deserves': 663,\n",
       " 'anyone': 664,\n",
       " 'tries': 665,\n",
       " 'show': 666,\n",
       " 'leftist': 667,\n",
       " 'view': 668,\n",
       " 'believe': 669,\n",
       " 'sock': 670,\n",
       " 'puppet': 671,\n",
       " 'doubt': 672,\n",
       " 'stick': 673,\n",
       " '2': 674,\n",
       " 'god': 675,\n",
       " 'dam': 676,\n",
       " 'battling': 677,\n",
       " 'hopeless': 678,\n",
       " 'idiots': 679,\n",
       " 'sympathy': 680,\n",
       " 'knowledge': 681,\n",
       " 'keep': 682,\n",
       " 'anticommunist': 683,\n",
       " 'untouched': 684,\n",
       " 'probably': 685,\n",
       " 'mistaken': 686,\n",
       " 'tried': 687,\n",
       " 'neutrally': 688,\n",
       " 'side': 689,\n",
       " 'history': 690,\n",
       " 'banned': 691,\n",
       " 'troubles': 692,\n",
       " 'own': 693,\n",
       " 'creations': 694,\n",
       " 'ignorance': 695,\n",
       " '2256': 696,\n",
       " 'opposition': 697,\n",
       " 'form': 698,\n",
       " 'creator': 699,\n",
       " 'including': 700,\n",
       " 'drugbox': 701,\n",
       " 'sulfur': 702,\n",
       " 'fields': 703,\n",
       " 'available': 704,\n",
       " 'elements': 705,\n",
       " 'infobox': 706,\n",
       " 'present': 707,\n",
       " 'pharmaceutical': 708,\n",
       " 'martyman': 709,\n",
       " 'sad': 710,\n",
       " 'man': 711,\n",
       " 'penis': 712,\n",
       " 'enjoys': 713,\n",
       " 'sucking': 714,\n",
       " 'love': 715,\n",
       " 'profile': 716,\n",
       " 'mr': 717,\n",
       " 'martin': 718,\n",
       " 'conway': 719,\n",
       " 'desperado': 720,\n",
       " 'rsvp': 721,\n",
       " 'written': 722,\n",
       " 'argument': 723,\n",
       " 'ah': 724,\n",
       " 'youre': 725,\n",
       " 'coming': 726,\n",
       " 'website': 727,\n",
       " 'suggested': 728,\n",
       " 'growing': 729,\n",
       " 'size': 730,\n",
       " 'cage': 731,\n",
       " 'environment': 732,\n",
       " 'seems': 733,\n",
       " 'stopped': 734,\n",
       " 'aside': 735,\n",
       " 'aforementioned': 736,\n",
       " 'tail': 737,\n",
       " 'confuses': 738,\n",
       " 'happened': 739,\n",
       " 'eight': 740,\n",
       " 'juveniles': 741,\n",
       " 'hundreds': 742,\n",
       " 'looking': 743,\n",
       " 'over': 744,\n",
       " 'nearly': 745,\n",
       " 'black': 746,\n",
       " 'color': 747,\n",
       " 'survived': 748,\n",
       " 'managed': 749,\n",
       " 'crawl': 750,\n",
       " 'corners': 751,\n",
       " 'die': 752,\n",
       " 'overnight': 753,\n",
       " 'half': 754,\n",
       " 'near': 755,\n",
       " 'length': 756,\n",
       " 'id': 757,\n",
       " 'guess': 758,\n",
       " 'somehow': 759,\n",
       " 'grow': 760,\n",
       " 'parallel': 761,\n",
       " '90degree': 762,\n",
       " 'angle': 763,\n",
       " 'tips': 764,\n",
       " 'rounded': 765,\n",
       " 'three': 766,\n",
       " 'questions': 767,\n",
       " 'knowledgeable': 768,\n",
       " 'subject': 769,\n",
       " 'turn': 770,\n",
       " 'green': 771,\n",
       " 'fix': 772,\n",
       " '1022': 773,\n",
       " '2007': 774,\n",
       " 'cheeseburger': 775,\n",
       " 'fatty': 776,\n",
       " 'attempt': 777,\n",
       " 'rfc': 778,\n",
       " 'option': 779,\n",
       " 'officially': 780,\n",
       " 'notifying': 781,\n",
       " 'hold': 782,\n",
       " 'block': 783,\n",
       " 'button': 784,\n",
       " 'figure': 785,\n",
       " 'almost': 786,\n",
       " 'shot': 787,\n",
       " 'weeping': 788,\n",
       " 'angels': 789,\n",
       " 'episode': 790,\n",
       " 'aviolate': 791,\n",
       " 'nfcc': 792,\n",
       " 'free': 793,\n",
       " 'bbe': 794,\n",
       " 'virtually': 795,\n",
       " 'indistinguishable': 796,\n",
       " 'episodes': 797,\n",
       " 'less': 798,\n",
       " 'identical': 799,\n",
       " 'tag': 800,\n",
       " 'woolsack': 801,\n",
       " 'ambiguous': 802,\n",
       " 'talking': 803,\n",
       " 'unless': 804,\n",
       " 'finally': 805,\n",
       " 'decide': 806,\n",
       " 'read': 807,\n",
       " 'replying': 808,\n",
       " 'request': 809,\n",
       " 'specific': 810,\n",
       " 'wrote': 811,\n",
       " 'above': 812,\n",
       " 'csssclll': 813,\n",
       " 'kitten': 814,\n",
       " 'soniyou': 815,\n",
       " 'great': 816,\n",
       " 'adopter': 817,\n",
       " 'helped': 818,\n",
       " 'right': 819,\n",
       " 'direction': 820,\n",
       " 'apologize': 821,\n",
       " 'obnoxious': 822,\n",
       " 'frustrated': 823,\n",
       " 'last': 824,\n",
       " 'night': 825,\n",
       " 'advice': 826,\n",
       " 'enjoy': 827,\n",
       " 'summer': 828,\n",
       " 'adoption': 829,\n",
       " 'come': 830,\n",
       " 'september': 831,\n",
       " 'refreshed': 832,\n",
       " 'youll': 833,\n",
       " 'deeply': 834,\n",
       " 'missed': 835,\n",
       " 'prabash': 836,\n",
       " 'community': 837,\n",
       " 'sucks': 838,\n",
       " 'yet': 839,\n",
       " 'asset': 840,\n",
       " 'wikipediagood': 841,\n",
       " 'luck': 842,\n",
       " 'wikibreak': 843,\n",
       " 'sincerely': 844,\n",
       " 'adoptee': 845,\n",
       " 'aka': 846,\n",
       " 'chat': 847,\n",
       " 'count': 848,\n",
       " 'remove': 849,\n",
       " 'afd': 850,\n",
       " 'tags': 851,\n",
       " 'wikipages': 852,\n",
       " 'considered': 853,\n",
       " 'vandalism': 854,\n",
       " 'talkcontribs': 855,\n",
       " 'tell': 856,\n",
       " 'cares': 857,\n",
       " 'contribute': 858,\n",
       " 'analysis': 859,\n",
       " 'western': 860,\n",
       " 'philosophies': 861,\n",
       " 'friends': 862,\n",
       " 'internet': 863,\n",
       " 'take': 864,\n",
       " 'hint': 865,\n",
       " 'ahole': 866,\n",
       " 'low': 867,\n",
       " 'selfesteem': 868,\n",
       " 'warn': 869,\n",
       " 'you4130134233': 870,\n",
       " 'speedily': 871,\n",
       " 'content': 872,\n",
       " 'copied': 873,\n",
       " 'source': 874,\n",
       " 'everything': 875,\n",
       " 'quoted': 876,\n",
       " 'cited': 877,\n",
       " 'references': 878,\n",
       " 'bottom': 879,\n",
       " 'creating': 880,\n",
       " 'joachim': 881,\n",
       " 'pissarro': 882,\n",
       " 'idea': 883,\n",
       " 'multiarticle': 884,\n",
       " 'subjects': 885,\n",
       " 'closely': 886,\n",
       " 'connected': 887,\n",
       " 'affected': 888,\n",
       " 'appropriate': 889,\n",
       " 'segments': 890,\n",
       " 'closed': 891,\n",
       " 'wouldnt': 892,\n",
       " 'hunt': 893,\n",
       " 'prior': 894,\n",
       " 'enduring': 895,\n",
       " 'verbal': 896,\n",
       " 'assaults': 897,\n",
       " 'following': 898,\n",
       " 'better': 899,\n",
       " 'concerned': 900,\n",
       " 'dealt': 901,\n",
       " 'numerous': 902,\n",
       " 'kinds': 903,\n",
       " 'structuring': 904,\n",
       " 'aspect': 905,\n",
       " 'drawing': 906,\n",
       " 'attention': 907,\n",
       " 'striking': 908,\n",
       " 'linear': 909,\n",
       " 'narrative': 910,\n",
       " 'aseptic': 911,\n",
       " 'surgical': 912,\n",
       " 'onesided': 913,\n",
       " 'arguments': 914,\n",
       " 'piled': 915,\n",
       " 'cv': 916,\n",
       " 'resum': 917,\n",
       " 'e': 918,\n",
       " 'failing': 919,\n",
       " 'extent': 920,\n",
       " 'actions': 921,\n",
       " 'presenting': 922,\n",
       " 'critical': 923,\n",
       " 'approaches': 924,\n",
       " 'positions': 925,\n",
       " 'severely': 926,\n",
       " 'critized': 927,\n",
       " 'many': 928,\n",
       " 'reputed': 929,\n",
       " 'authors': 930,\n",
       " 'groups': 931,\n",
       " 'lenghty': 932,\n",
       " 'criminal': 933,\n",
       " 'record': 934,\n",
       " 'alleged': 935,\n",
       " 'formal': 936,\n",
       " 'rigidity': 937,\n",
       " 'covers': 938,\n",
       " 'pentagon': 939,\n",
       " 'papers': 940,\n",
       " 'wondering': 941,\n",
       " 'share': 942,\n",
       " 'cohesiveness': 943,\n",
       " 'paragraph': 944,\n",
       " 'embedded': 945,\n",
       " 'refers': 946,\n",
       " 'vietnam': 947,\n",
       " 'context': 948,\n",
       " 'charge': 949,\n",
       " 'intended': 950,\n",
       " 'clear': 951,\n",
       " 'moment': 952,\n",
       " 'general': 953,\n",
       " 'person': 954,\n",
       " 'intend': 955,\n",
       " 'delete': 956,\n",
       " 'medalsmerits': 957,\n",
       " 'offer': 958,\n",
       " 'balanced': 959,\n",
       " 'approach': 960,\n",
       " 'completely': 961,\n",
       " 'different': 962,\n",
       " 'directly': 963,\n",
       " 'related': 964,\n",
       " 'reader': 965,\n",
       " 'picture': 966,\n",
       " 'domestic': 967,\n",
       " 'course': 968,\n",
       " 'events': 969,\n",
       " 'home': 970,\n",
       " 'eventually': 971,\n",
       " 'prompt': 972,\n",
       " 'usa': 973,\n",
       " 'pullout': 974,\n",
       " 'interacting': 975,\n",
       " 'accept': 976,\n",
       " 'suggestions': 977,\n",
       " 'collocation': 978,\n",
       " 'section': 979,\n",
       " 'question': 980,\n",
       " 'mending': 981,\n",
       " 'ellsberg': 982,\n",
       " 'highranking': 983,\n",
       " 'official': 984,\n",
       " 'proper': 985,\n",
       " 'word': 986,\n",
       " 'familiar': 987,\n",
       " 'midranking': 988,\n",
       " 'topsecret': 989,\n",
       " 'quotes': 990,\n",
       " 'telling': 991,\n",
       " 'revealed': 992,\n",
       " 'gets': 993,\n",
       " 'mirror': 994,\n",
       " 'position': 995,\n",
       " 'kissinger': 996,\n",
       " 'make': 997,\n",
       " 'based': 998,\n",
       " 'balance': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_to_ix))\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576331, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 6\n",
    "VOCAB_SIZE,NUM_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "!ls ../../vectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('../../vectors/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14257812, -0.03686523,  0.13574219, -0.06201172,  0.07958984,\n",
       "        0.01904297, -0.08154297, -0.12792969, -0.02954102,  0.23632812,\n",
       "       -0.12158203, -0.21484375,  0.12988281, -0.02709961, -0.05200195,\n",
       "        0.21582031, -0.18164062,  0.05102539, -0.16015625, -0.17675781,\n",
       "        0.01831055, -0.04125977, -0.23242188, -0.01031494,  0.14550781,\n",
       "        0.05249023, -0.39648438, -0.01928711,  0.0025177 , -0.01269531,\n",
       "       -0.04394531,  0.03076172,  0.09570312, -0.17578125,  0.01043701,\n",
       "        0.18945312, -0.23632812,  0.04370117,  0.28125   , -0.02075195,\n",
       "       -0.18164062, -0.21777344,  0.23339844,  0.05297852, -0.11376953,\n",
       "        0.00939941, -0.14941406,  0.19921875, -0.17578125,  0.31640625],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('test')[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM=300\n",
    "sd = 1/np.sqrt(W2V_DIM) ## standard deviation to use\n",
    "weights = np.random.normal(0, scale=sd, size=[VOCAB_SIZE, W2V_DIM])\n",
    "weights = weights.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_w2v_count = 0\n",
    "oov_words = []\n",
    "for word in word_to_ix:\n",
    "    id = word_to_ix.get(word)\n",
    "    #print(word,id)\n",
    "    if id is not None:\n",
    "        try:\n",
    "            weights[id]=w2v.word_vec(word)\n",
    "            #print(weights[id][0:20])\n",
    "            #weights[id]=glove_vector[word]\n",
    "        except:\n",
    "            #print(\"OOV word - {}\".format(word))\n",
    "            no_w2v_count +=1\n",
    "            oov_words.extend([word])\n",
    "            weights[id]=np.random.normal(0, scale=sd, size=[1, W2V_DIM]) ## If word not present, initialize randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Total vocabulary: 576331, OOV: 493654\n",
      ">> % of OOV words: 0.8565 %\n"
     ]
    }
   ],
   "source": [
    "print(\">> Total vocabulary: {}, OOV: {}\".format(len(word_to_ix),no_w2v_count))\n",
    "print(\">> % of OOV words: {:0.4f} %\".format(no_w2v_count/len(word_to_ix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - BoW Classifier - Bag of Embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifierEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, num_labels):\n",
    "        super(BoWClassifierEmbeddings, self).__init__()\n",
    "        \n",
    "        self.embeddings_bag = nn.EmbeddingBag(VOCAB_SIZE, W2V_DIM,scale_grad_by_freq=True, mode='sum')\n",
    "        self.embeddings_bag.weight.data=torch.Tensor(weights)\n",
    "        self.linear = nn.Linear(embedding_dim, num_labels)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "      \n",
    "        embeds = self.embeddings_bag(sentence,Variable(torch.LongTensor([0]).cuda()))\n",
    "        #linear = self.linear(embeds)\n",
    "        return F.log_softmax(self.linear(embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_embeddings(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq.split()]    \n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label):\n",
    "    return torch.LongTensor([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM = 300\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifierEmbeddings(\n",
       "  (embeddings_bag): EmbeddingBag(576331, 300, scale_grad_by_freq=True, mode=sum)\n",
       "  (linear): Linear(in_features=300, out_features=6)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BoWClassifierEmbeddings(embedding_dim = W2V_DIM,\n",
    "                        vocab_size=VOCAB_SIZE,\n",
    "                        num_labels=NUM_LABELS)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filmfare award for best male debutvenky that editor shshshis not updating the correct years for filmfare award for best male debut please check my addition done on 2325 17 june 2011 which is absolutely true please inform shshto update correct years also please check my talk in shsh page regarding same header\n",
      "Variable containing:\n",
      "  0\n",
      "  1\n",
      "  2\n",
      "  3\n",
      "  4\n",
      "  5\n",
      "  6\n",
      "  7\n",
      "  8\n",
      "  9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 13\n",
      "  2\n",
      "  0\n",
      "  1\n",
      "  2\n",
      "  3\n",
      "  4\n",
      " 14\n",
      " 15\n",
      " 16\n",
      " 17\n",
      " 18\n",
      " 19\n",
      " 20\n",
      " 21\n",
      " 22\n",
      " 23\n",
      " 24\n",
      " 25\n",
      " 26\n",
      " 27\n",
      " 28\n",
      " 15\n",
      " 29\n",
      " 30\n",
      " 31\n",
      " 12\n",
      " 13\n",
      " 32\n",
      " 15\n",
      " 16\n",
      " 17\n",
      " 33\n",
      " 34\n",
      " 35\n",
      " 36\n",
      " 37\n",
      " 38\n",
      " 39\n",
      "[torch.cuda.LongTensor of size 52 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "print(train_data[n][0])\n",
    "sample_phrase=Variable(make_sentence_embeddings(train_data[n][0],word_to_ix)).cuda()\n",
    "print(sample_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing Bag of Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_bag = nn.EmbeddingBag(VOCAB_SIZE, W2V_DIM, mode='mean')\n",
    "embeddings_bag.weight.data=torch.Tensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noted ill try to dig around a bit more to see if theres anything else that could be used to address this additionally ive added classical to the genre bit as per his choices and flowers release this makes me wonder if new age could also be used which would kind of help fill the search for something that details his music other than the widely applicable hiphop thoughts friend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  79\n",
       "  80\n",
       "  81\n",
       "  82\n",
       "  83\n",
       "  84\n",
       "  85\n",
       "  86\n",
       "  87\n",
       "  82\n",
       "  88\n",
       "  89\n",
       "  90\n",
       "  91\n",
       "  92\n",
       "   6\n",
       "  93\n",
       "  94\n",
       "  95\n",
       "  82\n",
       "  96\n",
       "  97\n",
       "  98\n",
       "  99\n",
       " 100\n",
       " 101\n",
       "  82\n",
       "  11\n",
       " 102\n",
       "  86\n",
       " 103\n",
       " 104\n",
       " 105\n",
       " 106\n",
       "  57\n",
       " 107\n",
       " 108\n",
       "  97\n",
       " 109\n",
       " 110\n",
       " 111\n",
       "  89\n",
       " 112\n",
       " 113\n",
       "  93\n",
       "  32\n",
       "  94\n",
       "  95\n",
       "  25\n",
       " 114\n",
       " 115\n",
       "  75\n",
       " 116\n",
       " 117\n",
       "  11\n",
       " 118\n",
       "   2\n",
       " 119\n",
       "   6\n",
       " 120\n",
       " 105\n",
       " 121\n",
       "  64\n",
       " 122\n",
       "  11\n",
       " 123\n",
       " 124\n",
       " 125\n",
       " 126\n",
       " 127\n",
       "[torch.cuda.LongTensor of size 70 (GPU 0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "print(train_data[n][0])\n",
    "sample_phrase=Variable(make_sentence_embeddings(train_data[n][0],word_to_ix)).cuda()\n",
    "sample_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fb3eb4c32dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_bag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_phrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlp_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlp_pytorch/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, offsets)\u001b[0m\n\u001b[1;32m    203\u001b[0m         return F.embedding_bag(self.weight, input, offsets,\n\u001b[1;32m    204\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                                self.scale_grad_by_freq, self.mode)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlp_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding_bag\u001b[0;34m(embedding_matrix, indices, offsets, max_norm, norm_type, scale_grad_by_freq, mode)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m     )\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlp_pytorch/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, weight, indices, offsets, max_norm, norm_type, scale_grad_by_freq, mode)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# slow CPU implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mindex_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;31m# indices = [1, 2, 30, 100, 12], offsets = [0, 2, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset2bag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# offset2bag = [0 0 0 0 0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)"
     ]
    }
   ],
   "source": [
    "offsets = Variable(torch.LongTensor([0])).cuda()\n",
    "embeds = embeddings_bag(sample_phrase, offsets)\n",
    "torch.sum(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(sample_phrase)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MultiLabelMarginLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "n_iters = 5000000\n",
    "num_epochs = n_iters/(len(x_train))/batch_size\n",
    "num_epochs=int(num_epochs)\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking incorrect predictions\n",
    "incorrect = 0\n",
    "incorrect_vect = pd.DataFrame(columns={'wrong_pred_intent', 'correct_intent','usersays'})\n",
    "incorrect_vect=incorrect_vect[['wrong_pred_intent', 'correct_intent','usersays']]\n",
    "incorrect_vect\n",
    "incorrect = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for (sent,label) in train_data:\n",
    "        # Step 1 - clear the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        ## Step 2- Prepare input and label\n",
    "        sample = Variable(make_sentence_embeddings(sent,word_to_ix)).cuda()\n",
    "        target = Variable(make_target(label)).cuda()\n",
    "    \n",
    "        # Step 3 - Run forward pass\n",
    "        output = model(sample)\n",
    "        #print(\"Log probabilities - {}\".format(log_probs))\n",
    "        \n",
    "        # Step 4 - Compute loss, gradients, update parameters\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter+=1      \n",
    "        ## Calculate final accuracy\n",
    "        if iter % 5000 ==0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (sent,label) in valid_data:\n",
    "                sample = Variable(make_sentence_embeddings(sent,word_to_ix)).cuda()\n",
    "                target = Variable(make_target(label)).cuda()\n",
    "                output = model(sample)\n",
    "                _,predicted = torch.max(output.data,1)\n",
    "                pred_label=list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted[0])]\n",
    "                total += target.size(0)\n",
    "                if (pred_label == label):\n",
    "                    correct += (predicted[0] == make_target(label, label_to_ix)).sum()\n",
    "                else:\n",
    "                    incorrect_vect.loc[incorrect] = [pred_label, label, sent]\n",
    "                    incorrect +=1\n",
    "            accuracy = 100 * correct/total\n",
    "            print('Iterations: {}. Loss: {}. Accuracy: {}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
