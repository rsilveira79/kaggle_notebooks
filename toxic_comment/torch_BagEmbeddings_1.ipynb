{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "## Torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## Sklearn imports\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss,roc_auc_score\n",
    "\n",
    "## NLP Libraries\n",
    "# Spacy\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "# NLTK\n",
    "from nltk import download\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.5\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 95851\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95846</th>\n",
       "      <td>999977655955</td>\n",
       "      <td>\"\\nI have discussed it, unlike most of those w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95847</th>\n",
       "      <td>999982426659</td>\n",
       "      <td>ps. Almost forgot, Paine don't reply back to t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95848</th>\n",
       "      <td>999982764066</td>\n",
       "      <td>Mamoun Darkazanli\\nFor some reason I am unable...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95849</th>\n",
       "      <td>999986890563</td>\n",
       "      <td>Salafi would be a better term. It is more poli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95850</th>\n",
       "      <td>999988164717</td>\n",
       "      <td>making wikipedia a better and more inviting pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "95846  999977655955  \"\\nI have discussed it, unlike most of those w...      0   \n",
       "95847  999982426659  ps. Almost forgot, Paine don't reply back to t...      1   \n",
       "95848  999982764066  Mamoun Darkazanli\\nFor some reason I am unable...      0   \n",
       "95849  999986890563  Salafi would be a better term. It is more poli...      0   \n",
       "95850  999988164717  making wikipedia a better and more inviting pl...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \n",
       "95846             0        0       0       0              0  \n",
       "95847             0        1       0       0              0  \n",
       "95848             0        0       0       0              0  \n",
       "95849             0        0       0       0              0  \n",
       "95850             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', delimiter=\",\")\n",
    "print(\"Train size: {}\".format(len(train)))\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 226998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226993</th>\n",
       "      <td>999966872214</td>\n",
       "      <td>*{Persondata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226994</th>\n",
       "      <td>999968525410</td>\n",
       "      <td>'' —  is wishing you a [WIKI_LINK: Mary Poppin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226995</th>\n",
       "      <td>999980053494</td>\n",
       "      <td>==Fair use rationale for [WIKI_LINK: Image:D.R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226996</th>\n",
       "      <td>999980680364</td>\n",
       "      <td>== Employment Practices at Majestic ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226997</th>\n",
       "      <td>999997819802</td>\n",
       "      <td>Welcome to Wikipedia. Although everyone is wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text\n",
       "226993  999966872214                                       *{Persondata\n",
       "226994  999968525410  '' —  is wishing you a [WIKI_LINK: Mary Poppin...\n",
       "226995  999980053494  ==Fair use rationale for [WIKI_LINK: Image:D.R...\n",
       "226996  999980680364             == Employment Practices at Majestic ==\n",
       "226997  999997819802  Welcome to Wikipedia. Although everyone is wel..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv', delimiter=\",\")\n",
    "print(\"Test size: {}\".format(len(test)))\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer_spacy(text):        \n",
    "    sent = []\n",
    "    doc = spacy_en(text)\n",
    "    #print(doc)\n",
    "    for word in doc:\n",
    "        if word.lemma_ == \"-PRON-\":\n",
    "            sent.append(word.text)\n",
    "        else:\n",
    "            sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    return ''.join(c for c in text if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, do_stop=False, do_lemma= False):\n",
    "    text = str(text)\n",
    "    #text = gensim.parsing.preprocessing.strip_numeric(text)  # Strip all the numerics\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) # Removing non ASCII chars\n",
    "    text = text.replace(\"\\n\",\" \") # Removing line breaks\n",
    "\n",
    "    # Remove the punctuation\n",
    "    text = strip_punctuation(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stops]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    ## Lemmatization\n",
    "    if (do_lemma==True):\n",
    "    #    text = lemmatizer_spacy(text)\n",
    "        text = lemmatizer.lemmatize(text) ## using NLTK lemmatizer\n",
    "        \n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)# Strip multiple whitespaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that is not cool'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"\\n ##?? %&that is not cool\"\n",
    "clean_text(msg, do_lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95846</th>\n",
       "      <td>999977655955</td>\n",
       "      <td>\"\\nI have discussed it, unlike most of those w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i have discussed it unlike most of those who r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95847</th>\n",
       "      <td>999982426659</td>\n",
       "      <td>ps. Almost forgot, Paine don't reply back to t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ps almost forgot paine dont reply back to this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95848</th>\n",
       "      <td>999982764066</td>\n",
       "      <td>Mamoun Darkazanli\\nFor some reason I am unable...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mamoun darkazanli for some reason i am unable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95849</th>\n",
       "      <td>999986890563</td>\n",
       "      <td>Salafi would be a better term. It is more poli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>salafi would be a better term it is more polit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95850</th>\n",
       "      <td>999988164717</td>\n",
       "      <td>making wikipedia a better and more inviting pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>making wikipedia a better and more inviting place</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "95846  999977655955  \"\\nI have discussed it, unlike most of those w...      0   \n",
       "95847  999982426659  ps. Almost forgot, Paine don't reply back to t...      1   \n",
       "95848  999982764066  Mamoun Darkazanli\\nFor some reason I am unable...      0   \n",
       "95849  999986890563  Salafi would be a better term. It is more poli...      0   \n",
       "95850  999988164717  making wikipedia a better and more inviting pl...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "95846             0        0       0       0              0   \n",
       "95847             0        1       0       0              0   \n",
       "95848             0        0       0       0              0   \n",
       "95849             0        0       0       0              0   \n",
       "95850             0        0       0       0              0   \n",
       "\n",
       "                                         cleaned_comment  \n",
       "95846  i have discussed it unlike most of those who r...  \n",
       "95847  ps almost forgot paine dont reply back to this...  \n",
       "95848  mamoun darkazanli for some reason i am unable ...  \n",
       "95849  salafi would be a better term it is more polit...  \n",
       "95850  making wikipedia a better and more inviting place  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_comment']=train['comment_text'].apply(lambda x:clean_text(x, do_lemma = True))\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "      <td>orphaned nonfree media image41cd1jboevl ss500 jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "      <td>kentuckiana is colloquial even though the area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "      <td>hello fellow wikipedians i have just modified ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "      <td>akc suspensions the morning call feb 24 2001 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "      <td>wikilink talkcelts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  \\\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...   \n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...   \n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...   \n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...   \n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] ==   \n",
       "\n",
       "                                     cleaned_comment  \n",
       "0  orphaned nonfree media image41cd1jboevl ss500 jpg  \n",
       "1  kentuckiana is colloquial even though the area...  \n",
       "2  hello fellow wikipedians i have just modified ...  \n",
       "3  akc suspensions the morning call feb 24 2001 7...  \n",
       "4                                 wikilink talkcelts  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['cleaned_comment']=test['comment_text'].apply(lambda x:clean_text(x, do_lemma=True))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(zip(train['toxic'], \n",
    "                    train['severe_toxic'],\n",
    "                    train['obscene'], \n",
    "                    train['threat'],\n",
    "                    train['insult'], \n",
    "                    train['identity_hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train['cleaned_comment'],\n",
    "                                                      labels, \n",
    "                                                      test_size=0.2,random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test['cleaned_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filmfare award for best male debut venky that editor shshshis not updating the correct years for filmfare award for best male debut please check my addition done on 2325 17 june 2011 which is absolutely true please inform shshto update correct years also please check my talk in shsh page regarding same header',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('charles sumner article hello dr jensen i have recently been making edits on the charles sumner article i have expanded on the dominican republic annexation treaty and information on president grant are there any other areas that need work on the cs article',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('nsa conspiracy theory please contain all discussion of the dubious sourcestatements here',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('noted ill try to dig around a bit more to see if theres anything else that could be used to address this additionally ive added classical to the genre bit as per his choices and flowers release this makes me wonder if new age could also be used which would kind of help fill the search for something that details his music other than the widely applicable hiphop thoughts friend',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('since apparently only one user would be so bold to say something like that about steve its a safe bet thats him especially considering he posted on the talk page of the ip that started this whole mess in the first place',\n",
       "  (0, 0, 0, 0, 0, 0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=list(zip(x_train,y_train))\n",
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reversing her early casual antisemitism when did this get added and where was it discussed meantime i have taken it out',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('i dont fix disambig by awb but just and it is working well',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('the phrase in europe it is refered to as white spirit was especially helpful for me it let me know that white spirit and mineral spirits are the same',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('fhu editing please explain to me how my editing of the freedhardeman university page was biased and not neutral preceding unsigned comment added by talk contribs',\n",
       "  (0, 0, 0, 0, 0, 0))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data=list(zip(x_valid,y_valid))\n",
    "valid_data[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build vocabulary of words\n",
    "word_to_ix = {}\n",
    "for (sent) in list(x_train) + list(x_valid) + list(x_test):\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filmfare': 0,\n",
       " 'award': 1,\n",
       " 'for': 2,\n",
       " 'best': 3,\n",
       " 'male': 4,\n",
       " 'debut': 5,\n",
       " 'venky': 6,\n",
       " 'that': 7,\n",
       " 'editor': 8,\n",
       " 'shshshis': 9,\n",
       " 'not': 10,\n",
       " 'updating': 11,\n",
       " 'the': 12,\n",
       " 'correct': 13,\n",
       " 'years': 14,\n",
       " 'please': 15,\n",
       " 'check': 16,\n",
       " 'my': 17,\n",
       " 'addition': 18,\n",
       " 'done': 19,\n",
       " 'on': 20,\n",
       " '2325': 21,\n",
       " '17': 22,\n",
       " 'june': 23,\n",
       " '2011': 24,\n",
       " 'which': 25,\n",
       " 'is': 26,\n",
       " 'absolutely': 27,\n",
       " 'true': 28,\n",
       " 'inform': 29,\n",
       " 'shshto': 30,\n",
       " 'update': 31,\n",
       " 'also': 32,\n",
       " 'talk': 33,\n",
       " 'in': 34,\n",
       " 'shsh': 35,\n",
       " 'page': 36,\n",
       " 'regarding': 37,\n",
       " 'same': 38,\n",
       " 'header': 39,\n",
       " 'charles': 40,\n",
       " 'sumner': 41,\n",
       " 'article': 42,\n",
       " 'hello': 43,\n",
       " 'dr': 44,\n",
       " 'jensen': 45,\n",
       " 'i': 46,\n",
       " 'have': 47,\n",
       " 'recently': 48,\n",
       " 'been': 49,\n",
       " 'making': 50,\n",
       " 'edits': 51,\n",
       " 'expanded': 52,\n",
       " 'dominican': 53,\n",
       " 'republic': 54,\n",
       " 'annexation': 55,\n",
       " 'treaty': 56,\n",
       " 'and': 57,\n",
       " 'information': 58,\n",
       " 'president': 59,\n",
       " 'grant': 60,\n",
       " 'are': 61,\n",
       " 'there': 62,\n",
       " 'any': 63,\n",
       " 'other': 64,\n",
       " 'areas': 65,\n",
       " 'need': 66,\n",
       " 'work': 67,\n",
       " 'cs': 68,\n",
       " 'nsa': 69,\n",
       " 'conspiracy': 70,\n",
       " 'theory': 71,\n",
       " 'contain': 72,\n",
       " 'all': 73,\n",
       " 'discussion': 74,\n",
       " 'of': 75,\n",
       " 'dubious': 76,\n",
       " 'sourcestatements': 77,\n",
       " 'here': 78,\n",
       " 'noted': 79,\n",
       " 'ill': 80,\n",
       " 'try': 81,\n",
       " 'to': 82,\n",
       " 'dig': 83,\n",
       " 'around': 84,\n",
       " 'a': 85,\n",
       " 'bit': 86,\n",
       " 'more': 87,\n",
       " 'see': 88,\n",
       " 'if': 89,\n",
       " 'theres': 90,\n",
       " 'anything': 91,\n",
       " 'else': 92,\n",
       " 'could': 93,\n",
       " 'be': 94,\n",
       " 'used': 95,\n",
       " 'address': 96,\n",
       " 'this': 97,\n",
       " 'additionally': 98,\n",
       " 'ive': 99,\n",
       " 'added': 100,\n",
       " 'classical': 101,\n",
       " 'genre': 102,\n",
       " 'as': 103,\n",
       " 'per': 104,\n",
       " 'his': 105,\n",
       " 'choices': 106,\n",
       " 'flowers': 107,\n",
       " 'release': 108,\n",
       " 'makes': 109,\n",
       " 'me': 110,\n",
       " 'wonder': 111,\n",
       " 'new': 112,\n",
       " 'age': 113,\n",
       " 'would': 114,\n",
       " 'kind': 115,\n",
       " 'help': 116,\n",
       " 'fill': 117,\n",
       " 'search': 118,\n",
       " 'something': 119,\n",
       " 'details': 120,\n",
       " 'music': 121,\n",
       " 'than': 122,\n",
       " 'widely': 123,\n",
       " 'applicable': 124,\n",
       " 'hiphop': 125,\n",
       " 'thoughts': 126,\n",
       " 'friend': 127,\n",
       " 'since': 128,\n",
       " 'apparently': 129,\n",
       " 'only': 130,\n",
       " 'one': 131,\n",
       " 'user': 132,\n",
       " 'so': 133,\n",
       " 'bold': 134,\n",
       " 'say': 135,\n",
       " 'like': 136,\n",
       " 'about': 137,\n",
       " 'steve': 138,\n",
       " 'its': 139,\n",
       " 'safe': 140,\n",
       " 'bet': 141,\n",
       " 'thats': 142,\n",
       " 'him': 143,\n",
       " 'especially': 144,\n",
       " 'considering': 145,\n",
       " 'he': 146,\n",
       " 'posted': 147,\n",
       " 'ip': 148,\n",
       " 'started': 149,\n",
       " 'whole': 150,\n",
       " 'mess': 151,\n",
       " 'first': 152,\n",
       " 'place': 153,\n",
       " 'agree': 154,\n",
       " 'nobody': 155,\n",
       " 'has': 156,\n",
       " 'write': 157,\n",
       " 'it': 158,\n",
       " 'should': 159,\n",
       " 'merged': 160,\n",
       " 'back': 161,\n",
       " '2005': 162,\n",
       " 'atlantic': 163,\n",
       " 'hurricane': 164,\n",
       " 'season': 165,\n",
       " 'reason': 166,\n",
       " 'nothing': 167,\n",
       " 'do': 168,\n",
       " 'with': 169,\n",
       " 'notability': 170,\n",
       " 'alpha': 171,\n",
       " 'wasnt': 172,\n",
       " 'way': 173,\n",
       " 'notable': 174,\n",
       " 'but': 175,\n",
       " 'point': 176,\n",
       " 'exist': 177,\n",
       " 'already': 178,\n",
       " 'exact': 179,\n",
       " 'thing': 180,\n",
       " 'summary': 181,\n",
       " 'shorter': 182,\n",
       " 'full': 183,\n",
       " 'text': 184,\n",
       " 'however': 185,\n",
       " 'no': 186,\n",
       " 'an': 187,\n",
       " 'edit': 188,\n",
       " 'war': 189,\n",
       " 'storm': 190,\n",
       " 'longer': 191,\n",
       " 'active': 192,\n",
       " 'decision': 193,\n",
       " 'isnt': 194,\n",
       " 'timesensitive': 195,\n",
       " 'suggestion': 196,\n",
       " 'add': 197,\n",
       " 'give': 198,\n",
       " 'people': 199,\n",
       " 'few': 200,\n",
       " 'weeks': 201,\n",
       " 'nightstallions': 202,\n",
       " 'wife': 203,\n",
       " 'got': 204,\n",
       " 'fucked': 205,\n",
       " 'by': 206,\n",
       " 'nigger': 207,\n",
       " 'had': 208,\n",
       " 'baby': 209,\n",
       " 'smelled': 210,\n",
       " 'fried': 211,\n",
       " 'chicken': 212,\n",
       " 'calling': 213,\n",
       " 'dickhead': 214,\n",
       " 'personal': 215,\n",
       " 'attack': 216,\n",
       " 'sorry': 217,\n",
       " 'fucks': 218,\n",
       " 'again': 219,\n",
       " 'will': 220,\n",
       " 'call': 221,\n",
       " 'utc': 222,\n",
       " 'unfortunately': 223,\n",
       " 'nonsollog': 224,\n",
       " 'get': 225,\n",
       " 'whacked': 226,\n",
       " '2324': 227,\n",
       " '18': 228,\n",
       " 'jan': 229,\n",
       " 'orphaned': 230,\n",
       " 'fair': 231,\n",
       " 'use': 232,\n",
       " 'image': 233,\n",
       " 'imagebtt': 234,\n",
       " 'uk': 235,\n",
       " '1jpg': 236,\n",
       " 'thanks': 237,\n",
       " 'uploading': 238,\n",
       " 'notice': 239,\n",
       " 'currently': 240,\n",
       " 'specifies': 241,\n",
       " 'unlicensed': 242,\n",
       " 'wikipedia': 243,\n",
       " 'may': 244,\n",
       " 'under': 245,\n",
       " 'claim': 246,\n",
       " 'meaning': 247,\n",
       " 'articles': 248,\n",
       " 'was': 249,\n",
       " 'previously': 250,\n",
       " 'go': 251,\n",
       " 'why': 252,\n",
       " 'removed': 253,\n",
       " 'you': 254,\n",
       " 'think': 255,\n",
       " 'useful': 256,\n",
       " 'note': 257,\n",
       " 'images': 258,\n",
       " 'replacement': 259,\n",
       " 'created': 260,\n",
       " 'acceptable': 261,\n",
       " 'our': 262,\n",
       " 'policy': 263,\n",
       " 'uploaded': 264,\n",
       " 'media': 265,\n",
       " 'whether': 266,\n",
       " 'theyre': 267,\n",
       " 'or': 268,\n",
       " 'can': 269,\n",
       " 'find': 270,\n",
       " 'list': 271,\n",
       " 'pages': 272,\n",
       " 'edited': 273,\n",
       " 'clicking': 274,\n",
       " 'contributions': 275,\n",
       " 'link': 276,\n",
       " 'located': 277,\n",
       " 'at': 278,\n",
       " 'very': 279,\n",
       " 'top': 280,\n",
       " 'when': 281,\n",
       " 'logged': 282,\n",
       " 'then': 283,\n",
       " 'selecting': 284,\n",
       " 'from': 285,\n",
       " 'dropdown': 286,\n",
       " 'box': 287,\n",
       " 'deleted': 288,\n",
       " 'after': 289,\n",
       " 'seven': 290,\n",
       " 'days': 291,\n",
       " 'described': 292,\n",
       " 'criteria': 293,\n",
       " 'speedy': 294,\n",
       " 'deletion': 295,\n",
       " 'thank': 296,\n",
       " 'contr': 297,\n",
       " 'fully': 298,\n",
       " 'vandalize': 299,\n",
       " 'did': 300,\n",
       " 'mary': 301,\n",
       " 'poppins': 302,\n",
       " 'continue': 303,\n",
       " 'blocked': 304,\n",
       " 'editing': 305,\n",
       " 'gabsadds': 306,\n",
       " 'havent': 307,\n",
       " 'responded': 308,\n",
       " 'recent': 309,\n",
       " 'messages': 310,\n",
       " 'doesnt': 311,\n",
       " 'matter': 312,\n",
       " 'anyway': 313,\n",
       " 'useressjay': 314,\n",
       " '24': 315,\n",
       " 'hours': 316,\n",
       " 'hes': 317,\n",
       " 'allergic': 318,\n",
       " 'phrase': 319,\n",
       " 'reply': 320,\n",
       " 'another': 321,\n",
       " 'admin': 322,\n",
       " 'seem': 323,\n",
       " 'understand': 324,\n",
       " 'exactly': 325,\n",
       " 'what': 326,\n",
       " 'means': 327,\n",
       " 'obviously': 328,\n",
       " 'entirely': 329,\n",
       " 'impressed': 330,\n",
       " 'being': 331,\n",
       " 'some': 332,\n",
       " 'even': 333,\n",
       " 'repeated': 334,\n",
       " 'attacks': 335,\n",
       " 'zyx': 336,\n",
       " 'language': 337,\n",
       " 'survey': 338,\n",
       " 'jericho': 339,\n",
       " 'harris': 340,\n",
       " 'poor': 341,\n",
       " 'examples': 342,\n",
       " 'look': 343,\n",
       " 'kevin': 344,\n",
       " 'nash': 345,\n",
       " 'hulk': 346,\n",
       " 'hogan': 347,\n",
       " 'scott': 348,\n",
       " 'hall': 349,\n",
       " 'jeff': 350,\n",
       " 'jarrett': 351,\n",
       " 'while': 352,\n",
       " 'they': 353,\n",
       " 'left': 354,\n",
       " 'wwfwcw': 355,\n",
       " 'their': 356,\n",
       " 'chronological': 357,\n",
       " 'order': 358,\n",
       " 'didnt': 359,\n",
       " 'anywhere': 360,\n",
       " 'endless': 361,\n",
       " 'dan': 362,\n",
       " 're': 363,\n",
       " 'bio': 364,\n",
       " 'templates': 365,\n",
       " 'comments': 366,\n",
       " 'corrected': 367,\n",
       " 'solomons': 368,\n",
       " 'seychelles': 369,\n",
       " 'comoros': 370,\n",
       " 'deliberatly': 371,\n",
       " 'out': 372,\n",
       " 'macau': 373,\n",
       " 'number': 374,\n",
       " 'countries': 375,\n",
       " 'know': 376,\n",
       " 'past': 377,\n",
       " 'discussions': 378,\n",
       " 'taken': 379,\n",
       " 'country': 380,\n",
       " 'decided': 381,\n",
       " 'wave': 382,\n",
       " 'play': 383,\n",
       " 'member': 384,\n",
       " 'un': 385,\n",
       " 'well': 386,\n",
       " 'mmove': 387,\n",
       " 'others': 388,\n",
       " 'later': 389,\n",
       " 'such': 390,\n",
       " 'gibralter': 391,\n",
       " 'vatican': 392,\n",
       " 'city': 393,\n",
       " 'etc': 394,\n",
       " 'now': 395,\n",
       " 'enough': 396,\n",
       " 'neiln': 397,\n",
       " 'every': 398,\n",
       " 'real': 399,\n",
       " 'ones': 400,\n",
       " 'truth': 401,\n",
       " 'let': 402,\n",
       " 'straight': 403,\n",
       " '4': 404,\n",
       " 'oclock': 405,\n",
       " 'morning': 406,\n",
       " 'hour': 407,\n",
       " 'sane': 408,\n",
       " 'bed': 409,\n",
       " 'revert': 410,\n",
       " 'changes': 411,\n",
       " 'your': 412,\n",
       " 'buddy': 413,\n",
       " 'sergecross': 414,\n",
       " 'threatened': 415,\n",
       " 'badlyspelled': 416,\n",
       " 'screed': 417,\n",
       " 'pointing': 418,\n",
       " 'naughty': 419,\n",
       " 'boy': 420,\n",
       " 'am': 421,\n",
       " 'ok': 422,\n",
       " 'whats': 423,\n",
       " 'going': 424,\n",
       " 'wikistalking': 425,\n",
       " 'provocation': 426,\n",
       " 'hope': 427,\n",
       " 'angry': 428,\n",
       " 'names': 429,\n",
       " 'worst': 430,\n",
       " 'crime': 431,\n",
       " 'ever': 432,\n",
       " 'rolls': 433,\n",
       " 'fainting': 434,\n",
       " 'couch': 435,\n",
       " 'proceeds': 436,\n",
       " 'ban': 437,\n",
       " 'most': 438,\n",
       " 'remaining': 439,\n",
       " 'millenium': 440,\n",
       " 'justice': 441,\n",
       " 'much': 442,\n",
       " 'rejoicing': 443,\n",
       " 'tell': 444,\n",
       " 'ya': 445,\n",
       " 'change': 446,\n",
       " 'indeed': 447,\n",
       " 'anybodys': 448,\n",
       " 'into': 449,\n",
       " 'want': 450,\n",
       " 'translate': 451,\n",
       " 'them': 452,\n",
       " 'serbocroatian': 453,\n",
       " 'substitute': 454,\n",
       " 'sam': 455,\n",
       " 'chowderhead': 456,\n",
       " 'george': 457,\n",
       " 'w': 458,\n",
       " 'bush': 459,\n",
       " 'world': 460,\n",
       " 'football': 461,\n",
       " 'league': 462,\n",
       " 'marxist': 463,\n",
       " 'treatise': 464,\n",
       " 'asserts': 465,\n",
       " 'washington': 466,\n",
       " 'dick': 467,\n",
       " 'clark': 468,\n",
       " 'were': 469,\n",
       " 'gay': 470,\n",
       " 'lovers': 471,\n",
       " 'christmas': 472,\n",
       " 'shall': 473,\n",
       " 'charitable': 474,\n",
       " 'youre': 475,\n",
       " 'welcome': 476,\n",
       " 'either': 477,\n",
       " 'drinking': 478,\n",
       " 'too': 479,\n",
       " 'caffenine': 480,\n",
       " 'b': 481,\n",
       " 'taking': 482,\n",
       " 'drugs': 483,\n",
       " 'c': 484,\n",
       " 'just': 485,\n",
       " 'felt': 486,\n",
       " 'loosing': 487,\n",
       " 'mind': 488,\n",
       " 'purpose': 489,\n",
       " 'tech': 490,\n",
       " 'escape': 491,\n",
       " 'orbit': 492,\n",
       " 'yeah': 493,\n",
       " 'looks': 494,\n",
       " 'uncivil': 495,\n",
       " 'because': 496,\n",
       " 'getting': 497,\n",
       " 'otherwise': 498,\n",
       " 'simply': 499,\n",
       " 'put': 500,\n",
       " 'moron': 501,\n",
       " 'yes': 502,\n",
       " 'assuming': 503,\n",
       " 'said': 504,\n",
       " 'never': 505,\n",
       " 'how': 506,\n",
       " 'morons': 507,\n",
       " 'hence': 508,\n",
       " 'description': 509,\n",
       " 'tone': 510,\n",
       " 'least': 511,\n",
       " 'combative': 512,\n",
       " 'mine': 513,\n",
       " 'who': 514,\n",
       " 'needs': 515,\n",
       " 'calm': 516,\n",
       " 'down': 517,\n",
       " 'actually': 518,\n",
       " 'dont': 519,\n",
       " 'quite': 520,\n",
       " '3rr': 521,\n",
       " 'works': 522,\n",
       " 'someone': 523,\n",
       " 'reverts': 524,\n",
       " '3': 525,\n",
       " 'times': 526,\n",
       " 'worth': 527,\n",
       " 'time': 528,\n",
       " 'broken': 529,\n",
       " 'until': 530,\n",
       " 'guys': 531,\n",
       " 'team': 532,\n",
       " 'consensus': 533,\n",
       " 'despite': 534,\n",
       " 'im': 535,\n",
       " 'part': 536,\n",
       " 'editors': 537,\n",
       " 'dispute': 538,\n",
       " 'still': 539,\n",
       " 'answered': 540,\n",
       " 'supposed': 541,\n",
       " 'gone': 542,\n",
       " 'deeper': 543,\n",
       " 'satisfied': 544,\n",
       " 'before': 545,\n",
       " 'perhaps': 546,\n",
       " 'ask': 547,\n",
       " 'doing': 548,\n",
       " 'fakesmile': 549,\n",
       " 'fact': 550,\n",
       " 'she': 551,\n",
       " 'editwarring': 552,\n",
       " 'shes': 553,\n",
       " 'mean': 554,\n",
       " 'issue': 555,\n",
       " 'okay': 556,\n",
       " 'reversion': 557,\n",
       " 'junk': 558,\n",
       " 'where': 559,\n",
       " 'stylized': 560,\n",
       " 'ebay': 561,\n",
       " 'without': 562,\n",
       " 'old': 563,\n",
       " 'version': 564,\n",
       " 'up': 565,\n",
       " 'bullets': 566,\n",
       " 'reversions': 567,\n",
       " 'those': 568,\n",
       " 'early': 569,\n",
       " 'little': 570,\n",
       " 'group': 571,\n",
       " 'called': 572,\n",
       " 'originally': 573,\n",
       " 'reverting': 574,\n",
       " 'leaving': 575,\n",
       " 'mention': 576,\n",
       " 'current': 577,\n",
       " 'style': 578,\n",
       " 'lead': 579,\n",
       " 'according': 580,\n",
       " 'earlier': 581,\n",
       " 'saying': 582,\n",
       " 'mentioning': 583,\n",
       " 'former': 584,\n",
       " 'erase': 585,\n",
       " 'both': 586,\n",
       " 'erasing': 587,\n",
       " 'though': 588,\n",
       " 'original': 589,\n",
       " 'concern': 590,\n",
       " 'against': 591,\n",
       " '7516221181': 592,\n",
       " 'april': 593,\n",
       " '2008': 594,\n",
       " 'heres': 595,\n",
       " 'really': 596,\n",
       " 'problem': 597,\n",
       " 'u': 598,\n",
       " 'ppl': 599,\n",
       " 'shit': 600,\n",
       " 'theirs': 601,\n",
       " 'metallica': 602,\n",
       " 'pissed': 603,\n",
       " 'off': 604,\n",
       " 'we': 605,\n",
       " 'breaking': 606,\n",
       " 'points': 607,\n",
       " 'happens': 608,\n",
       " 'weaker': 609,\n",
       " 'urs': 610,\n",
       " 'admit': 611,\n",
       " 'metal': 612,\n",
       " 'abot': 613,\n",
       " 'alot': 614,\n",
       " 'happy': 615,\n",
       " 'area': 616,\n",
       " 'went': 617,\n",
       " 'far': 618,\n",
       " 'mile': 619,\n",
       " 'suggesting': 620,\n",
       " 'end': 621,\n",
       " 'insult': 622,\n",
       " 'twice': 623,\n",
       " 'bad': 624,\n",
       " 'untill': 625,\n",
       " 'words': 626,\n",
       " 'rest': 627,\n",
       " 'wont': 628,\n",
       " 'prove': 629,\n",
       " 'each': 630,\n",
       " 'life': 631,\n",
       " 'problems': 632,\n",
       " 'bother': 633,\n",
       " 'wrong': 634,\n",
       " 'abit': 635,\n",
       " 'defence': 636,\n",
       " 'deserve': 637,\n",
       " 'iv': 638,\n",
       " 'deserved': 639,\n",
       " 'body': 640,\n",
       " 'kicked': 641,\n",
       " 'arse': 642,\n",
       " 'gonna': 643,\n",
       " 'happen': 644,\n",
       " 'soon': 645,\n",
       " 'stupid': 646,\n",
       " 'stop': 647,\n",
       " 'good': 648,\n",
       " 'whatever': 649,\n",
       " 'stoping': 650,\n",
       " 'won': 651,\n",
       " 'win': 652,\n",
       " 'cheers': 653,\n",
       " '0044': 654,\n",
       " '15': 655,\n",
       " '13': 656,\n",
       " 'november': 657,\n",
       " '2009': 658,\n",
       " 'utcwhat': 659,\n",
       " 'jacob': 660,\n",
       " 'peters': 661,\n",
       " 'deserves': 662,\n",
       " 'anyone': 663,\n",
       " 'tries': 664,\n",
       " 'show': 665,\n",
       " 'leftist': 666,\n",
       " 'view': 667,\n",
       " 'believe': 668,\n",
       " 'sock': 669,\n",
       " 'puppet': 670,\n",
       " 'doubt': 671,\n",
       " 'stick': 672,\n",
       " '2': 673,\n",
       " 'god': 674,\n",
       " 'dam': 675,\n",
       " 'battling': 676,\n",
       " 'hopeless': 677,\n",
       " 'idiots': 678,\n",
       " 'sympathy': 679,\n",
       " 'knowledge': 680,\n",
       " 'keep': 681,\n",
       " 'anticommunist': 682,\n",
       " 'untouched': 683,\n",
       " 'probably': 684,\n",
       " 'mistaken': 685,\n",
       " 'tried': 686,\n",
       " 'neutrally': 687,\n",
       " 'side': 688,\n",
       " 'history': 689,\n",
       " 'banned': 690,\n",
       " 'troubles': 691,\n",
       " 'own': 692,\n",
       " 'creations': 693,\n",
       " 'ignorance': 694,\n",
       " '2256': 695,\n",
       " 'opposition': 696,\n",
       " 'form': 697,\n",
       " 'creator': 698,\n",
       " 'including': 699,\n",
       " 'drugbox': 700,\n",
       " 'sulfur': 701,\n",
       " 'fields': 702,\n",
       " 'available': 703,\n",
       " 'elements': 704,\n",
       " 'infobox': 705,\n",
       " 'present': 706,\n",
       " 'pharmaceutical': 707,\n",
       " 'martyman': 708,\n",
       " 'sad': 709,\n",
       " 'man': 710,\n",
       " 'penis': 711,\n",
       " 'enjoys': 712,\n",
       " 'sucking': 713,\n",
       " 'love': 714,\n",
       " 'profile': 715,\n",
       " 'mr': 716,\n",
       " 'martin': 717,\n",
       " 'conway': 718,\n",
       " 'desperado': 719,\n",
       " 'rsvp': 720,\n",
       " 'written': 721,\n",
       " 'argument': 722,\n",
       " 'ah': 723,\n",
       " 'coming': 724,\n",
       " 'website': 725,\n",
       " 'suggested': 726,\n",
       " 'growing': 727,\n",
       " 'size': 728,\n",
       " 'cage': 729,\n",
       " 'environment': 730,\n",
       " 'seems': 731,\n",
       " 'stopped': 732,\n",
       " 'aside': 733,\n",
       " 'aforementioned': 734,\n",
       " 'tail': 735,\n",
       " 'confuses': 736,\n",
       " 'happened': 737,\n",
       " 'eight': 738,\n",
       " 'juveniles': 739,\n",
       " 'hundreds': 740,\n",
       " 'looking': 741,\n",
       " 'over': 742,\n",
       " 'nearly': 743,\n",
       " 'black': 744,\n",
       " 'color': 745,\n",
       " 'survived': 746,\n",
       " 'managed': 747,\n",
       " 'crawl': 748,\n",
       " 'corners': 749,\n",
       " 'die': 750,\n",
       " 'overnight': 751,\n",
       " 'half': 752,\n",
       " 'near': 753,\n",
       " 'length': 754,\n",
       " 'id': 755,\n",
       " 'guess': 756,\n",
       " 'somehow': 757,\n",
       " 'grow': 758,\n",
       " 'parallel': 759,\n",
       " '90degree': 760,\n",
       " 'angle': 761,\n",
       " 'tips': 762,\n",
       " 'rounded': 763,\n",
       " 'three': 764,\n",
       " 'questions': 765,\n",
       " 'knowledgeable': 766,\n",
       " 'subject': 767,\n",
       " 'turn': 768,\n",
       " 'green': 769,\n",
       " 'fix': 770,\n",
       " '1022': 771,\n",
       " '2007': 772,\n",
       " 'cheeseburger': 773,\n",
       " 'fatty': 774,\n",
       " 'attempt': 775,\n",
       " 'rfc': 776,\n",
       " 'option': 777,\n",
       " 'officially': 778,\n",
       " 'notifying': 779,\n",
       " 'hold': 780,\n",
       " 'block': 781,\n",
       " 'button': 782,\n",
       " 'figure': 783,\n",
       " 'almost': 784,\n",
       " 'shot': 785,\n",
       " 'weeping': 786,\n",
       " 'angels': 787,\n",
       " 'episode': 788,\n",
       " 'aviolate': 789,\n",
       " 'nfcc': 790,\n",
       " 'free': 791,\n",
       " 'bbe': 792,\n",
       " 'virtually': 793,\n",
       " 'indistinguishable': 794,\n",
       " 'episodes': 795,\n",
       " 'less': 796,\n",
       " 'identical': 797,\n",
       " 'tag': 798,\n",
       " 'woolsack': 799,\n",
       " 'ambiguous': 800,\n",
       " 'talking': 801,\n",
       " 'unless': 802,\n",
       " 'finally': 803,\n",
       " 'decide': 804,\n",
       " 'read': 805,\n",
       " 'replying': 806,\n",
       " 'request': 807,\n",
       " 'specific': 808,\n",
       " 'wrote': 809,\n",
       " 'above': 810,\n",
       " 'csssclll': 811,\n",
       " 'kitten': 812,\n",
       " 'soni': 813,\n",
       " 'great': 814,\n",
       " 'adopter': 815,\n",
       " 'helped': 816,\n",
       " 'right': 817,\n",
       " 'direction': 818,\n",
       " 'apologize': 819,\n",
       " 'obnoxious': 820,\n",
       " 'frustrated': 821,\n",
       " 'last': 822,\n",
       " 'night': 823,\n",
       " 'advice': 824,\n",
       " 'enjoy': 825,\n",
       " 'summer': 826,\n",
       " 'adoption': 827,\n",
       " 'come': 828,\n",
       " 'september': 829,\n",
       " 'refreshed': 830,\n",
       " 'youll': 831,\n",
       " 'deeply': 832,\n",
       " 'missed': 833,\n",
       " 'prabash': 834,\n",
       " 'community': 835,\n",
       " 'sucks': 836,\n",
       " 'yet': 837,\n",
       " 'asset': 838,\n",
       " 'luck': 839,\n",
       " 'wikibreak': 840,\n",
       " 'sincerely': 841,\n",
       " 'adoptee': 842,\n",
       " 'aka': 843,\n",
       " 'chat': 844,\n",
       " 'count': 845,\n",
       " 'remove': 846,\n",
       " 'afd': 847,\n",
       " 'tags': 848,\n",
       " 'wikipages': 849,\n",
       " 'considered': 850,\n",
       " 'vandalism': 851,\n",
       " 'talkcontribs': 852,\n",
       " 'cares': 853,\n",
       " 'contribute': 854,\n",
       " 'analysis': 855,\n",
       " 'western': 856,\n",
       " 'philosophies': 857,\n",
       " 'friends': 858,\n",
       " 'internet': 859,\n",
       " 'take': 860,\n",
       " 'hint': 861,\n",
       " 'ahole': 862,\n",
       " 'low': 863,\n",
       " 'selfesteem': 864,\n",
       " 'warn': 865,\n",
       " 'you4130134233': 866,\n",
       " 'speedily': 867,\n",
       " 'content': 868,\n",
       " 'copied': 869,\n",
       " 'source': 870,\n",
       " 'everything': 871,\n",
       " 'quoted': 872,\n",
       " 'cited': 873,\n",
       " 'references': 874,\n",
       " 'bottom': 875,\n",
       " 'creating': 876,\n",
       " 'joachim': 877,\n",
       " 'pissarro': 878,\n",
       " 'idea': 879,\n",
       " 'multiarticle': 880,\n",
       " 'subjects': 881,\n",
       " 'closely': 882,\n",
       " 'connected': 883,\n",
       " 'affected': 884,\n",
       " 'appropriate': 885,\n",
       " 'segments': 886,\n",
       " 'closed': 887,\n",
       " 'wouldnt': 888,\n",
       " 'hunt': 889,\n",
       " 'prior': 890,\n",
       " 'enduring': 891,\n",
       " 'verbal': 892,\n",
       " 'assaults': 893,\n",
       " 'following': 894,\n",
       " 'better': 895,\n",
       " 'concerned': 896,\n",
       " 'dealt': 897,\n",
       " 'numerous': 898,\n",
       " 'kinds': 899,\n",
       " 'structuring': 900,\n",
       " 'aspect': 901,\n",
       " 'drawing': 902,\n",
       " 'attention': 903,\n",
       " 'striking': 904,\n",
       " 'linear': 905,\n",
       " 'narrative': 906,\n",
       " 'aseptic': 907,\n",
       " 'surgical': 908,\n",
       " 'onesided': 909,\n",
       " 'arguments': 910,\n",
       " 'piled': 911,\n",
       " 'cv': 912,\n",
       " 'resum': 913,\n",
       " 'e': 914,\n",
       " 'failing': 915,\n",
       " 'extent': 916,\n",
       " 'actions': 917,\n",
       " 'presenting': 918,\n",
       " 'critical': 919,\n",
       " 'approaches': 920,\n",
       " 'positions': 921,\n",
       " 'severely': 922,\n",
       " 'critized': 923,\n",
       " 'many': 924,\n",
       " 'reputed': 925,\n",
       " 'authors': 926,\n",
       " 'groups': 927,\n",
       " 'lenghty': 928,\n",
       " 'criminal': 929,\n",
       " 'record': 930,\n",
       " 'alleged': 931,\n",
       " 'formal': 932,\n",
       " 'rigidity': 933,\n",
       " 'covers': 934,\n",
       " 'pentagon': 935,\n",
       " 'papers': 936,\n",
       " 'wondering': 937,\n",
       " 'share': 938,\n",
       " 'cohesiveness': 939,\n",
       " 'paragraph': 940,\n",
       " 'embedded': 941,\n",
       " 'refers': 942,\n",
       " 'vietnam': 943,\n",
       " 'context': 944,\n",
       " 'charge': 945,\n",
       " 'intended': 946,\n",
       " 'clear': 947,\n",
       " 'moment': 948,\n",
       " 'general': 949,\n",
       " 'person': 950,\n",
       " 'intend': 951,\n",
       " 'delete': 952,\n",
       " 'medalsmerits': 953,\n",
       " 'offer': 954,\n",
       " 'balanced': 955,\n",
       " 'approach': 956,\n",
       " 'completely': 957,\n",
       " 'different': 958,\n",
       " 'directly': 959,\n",
       " 'related': 960,\n",
       " 'reader': 961,\n",
       " 'picture': 962,\n",
       " 'domestic': 963,\n",
       " 'course': 964,\n",
       " 'events': 965,\n",
       " 'home': 966,\n",
       " 'eventually': 967,\n",
       " 'prompt': 968,\n",
       " 'usa': 969,\n",
       " 'pullout': 970,\n",
       " 'interacting': 971,\n",
       " 'accept': 972,\n",
       " 'suggestions': 973,\n",
       " 'collocation': 974,\n",
       " 'section': 975,\n",
       " 'question': 976,\n",
       " 'mending': 977,\n",
       " 'ellsberg': 978,\n",
       " 'highranking': 979,\n",
       " 'official': 980,\n",
       " 'proper': 981,\n",
       " 'word': 982,\n",
       " 'familiar': 983,\n",
       " 'midranking': 984,\n",
       " 'topsecret': 985,\n",
       " 'quotes': 986,\n",
       " 'telling': 987,\n",
       " 'revealed': 988,\n",
       " 'gets': 989,\n",
       " 'mirror': 990,\n",
       " 'position': 991,\n",
       " 'kissinger': 992,\n",
       " 'make': 993,\n",
       " 'based': 994,\n",
       " 'balance': 995,\n",
       " 'realistic': 996,\n",
       " 'reality': 997,\n",
       " 'dirtier': 998,\n",
       " 'told': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_to_ix))\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468842, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 6\n",
    "VOCAB_SIZE,NUM_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "!ls ../../vectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('../../vectors/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14257812, -0.03686523,  0.13574219, -0.06201172,  0.07958984,\n",
       "        0.01904297, -0.08154297, -0.12792969, -0.02954102,  0.23632812,\n",
       "       -0.12158203, -0.21484375,  0.12988281, -0.02709961, -0.05200195,\n",
       "        0.21582031, -0.18164062,  0.05102539, -0.16015625, -0.17675781,\n",
       "        0.01831055, -0.04125977, -0.23242188, -0.01031494,  0.14550781,\n",
       "        0.05249023, -0.39648438, -0.01928711,  0.0025177 , -0.01269531,\n",
       "       -0.04394531,  0.03076172,  0.09570312, -0.17578125,  0.01043701,\n",
       "        0.18945312, -0.23632812,  0.04370117,  0.28125   , -0.02075195,\n",
       "       -0.18164062, -0.21777344,  0.23339844,  0.05297852, -0.11376953,\n",
       "        0.00939941, -0.14941406,  0.19921875, -0.17578125,  0.31640625],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('test')[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM=300\n",
    "sd = 1/np.sqrt(W2V_DIM) ## standard deviation to use\n",
    "weights = np.random.normal(0, scale=sd, size=[VOCAB_SIZE, W2V_DIM])\n",
    "weights = weights.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_w2v_count = 0\n",
    "oov_words = []\n",
    "for word in word_to_ix:\n",
    "    id = word_to_ix.get(word)\n",
    "    #print(word,id)\n",
    "    if id is not None:\n",
    "        try:\n",
    "            weights[id]=w2v.word_vec(word)\n",
    "            #print(weights[id][0:20])\n",
    "            #weights[id]=glove_vector[word]\n",
    "        except:\n",
    "            #print(\"OOV word - {}\".format(word))\n",
    "            no_w2v_count +=1\n",
    "            oov_words.extend([word])\n",
    "            weights[id]=np.random.normal(0, scale=sd, size=[1, W2V_DIM]) ## If word not present, initialize randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Total vocabulary: 468842, OOV: 385950\n",
      ">> % of OOV words: 0.8232 %\n"
     ]
    }
   ],
   "source": [
    "print(\">> Total vocabulary: {}, OOV: {}\".format(len(word_to_ix),no_w2v_count))\n",
    "print(\">> % of OOV words: {:0.4f} %\".format(no_w2v_count/len(word_to_ix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - BoW Classifier - Bag of Embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifierEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, num_labels):\n",
    "        super(BoWClassifierEmbeddings, self).__init__()\n",
    "        \n",
    "        self.embeddings_bag = nn.EmbeddingBag(VOCAB_SIZE, W2V_DIM,scale_grad_by_freq=True, mode='sum')\n",
    "        self.embeddings_bag.weight.data=torch.Tensor(weights)\n",
    "        self.linear = nn.Linear(embedding_dim, num_labels)\n",
    "        self.softmax = nn.Softmax(dim=None)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "      \n",
    "        embeds = self.embeddings_bag(sentence,Variable(torch.LongTensor([0]).cuda()))\n",
    "        linear = self.linear(embeds)\n",
    "        log_soft = F.log_softmax(linear, dim=None)\n",
    "        out = self.softmax(log_soft)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_embeddings(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq.split()]    \n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label):\n",
    "    return torch.LongTensor([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_DIM = 300\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifierEmbeddings(\n",
       "  (embeddings_bag): EmbeddingBag(468842, 300, scale_grad_by_freq=True, mode=sum)\n",
       "  (linear): Linear(in_features=300, out_features=6)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BoWClassifierEmbeddings(embedding_dim = W2V_DIM,\n",
    "                        vocab_size=VOCAB_SIZE,\n",
    "                        num_labels=NUM_LABELS)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filmfare award for best male debut venky that editor shshshis not updating the correct years for filmfare award for best male debut please check my addition done on 2325 17 june 2011 which is absolutely true please inform shshto update correct years also please check my talk in shsh page regarding same header\n",
      "Variable containing:\n",
      "  0\n",
      "  1\n",
      "  2\n",
      "  3\n",
      "  4\n",
      "  5\n",
      "  6\n",
      "  7\n",
      "  8\n",
      "  9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 13\n",
      " 14\n",
      "  2\n",
      "  0\n",
      "  1\n",
      "  2\n",
      "  3\n",
      "  4\n",
      "  5\n",
      " 15\n",
      " 16\n",
      " 17\n",
      " 18\n",
      " 19\n",
      " 20\n",
      " 21\n",
      " 22\n",
      " 23\n",
      " 24\n",
      " 25\n",
      " 26\n",
      " 27\n",
      " 28\n",
      " 15\n",
      " 29\n",
      " 30\n",
      " 31\n",
      " 13\n",
      " 14\n",
      " 32\n",
      " 15\n",
      " 16\n",
      " 17\n",
      " 33\n",
      " 34\n",
      " 35\n",
      " 36\n",
      " 37\n",
      " 38\n",
      " 39\n",
      "[torch.cuda.LongTensor of size 53 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "print(train_data[n][0])\n",
    "sample_phrase=Variable(make_sentence_embeddings(train_data[n][0],word_to_ix)).cuda()\n",
    "print(sample_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing Bag of Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_bag = nn.EmbeddingBag(VOCAB_SIZE, W2V_DIM, mode='mean')\n",
    "embeddings_bag.weight.data=torch.Tensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noted ill try to dig around a bit more to see if theres anything else that could be used to address this additionally ive added classical to the genre bit as per his choices and flowers release this makes me wonder if new age could also be used which would kind of help fill the search for something that details his music other than the widely applicable hiphop thoughts friend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  79\n",
       "  80\n",
       "  81\n",
       "  82\n",
       "  83\n",
       "  84\n",
       "  85\n",
       "  86\n",
       "  87\n",
       "  82\n",
       "  88\n",
       "  89\n",
       "  90\n",
       "  91\n",
       "  92\n",
       "   7\n",
       "  93\n",
       "  94\n",
       "  95\n",
       "  82\n",
       "  96\n",
       "  97\n",
       "  98\n",
       "  99\n",
       " 100\n",
       " 101\n",
       "  82\n",
       "  12\n",
       " 102\n",
       "  86\n",
       " 103\n",
       " 104\n",
       " 105\n",
       " 106\n",
       "  57\n",
       " 107\n",
       " 108\n",
       "  97\n",
       " 109\n",
       " 110\n",
       " 111\n",
       "  89\n",
       " 112\n",
       " 113\n",
       "  93\n",
       "  32\n",
       "  94\n",
       "  95\n",
       "  25\n",
       " 114\n",
       " 115\n",
       "  75\n",
       " 116\n",
       " 117\n",
       "  12\n",
       " 118\n",
       "   2\n",
       " 119\n",
       "   7\n",
       " 120\n",
       " 105\n",
       " 121\n",
       "  64\n",
       " 122\n",
       "  12\n",
       " 123\n",
       " 124\n",
       " 125\n",
       " 126\n",
       " 127\n",
       "[torch.cuda.LongTensor of size 70 (GPU 0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "print(train_data[n][0])\n",
    "sample_phrase=Variable(make_sentence_embeddings(train_data[n][0],word_to_ix)).cuda()\n",
    "sample_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Envs/nlp_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "/home/ubuntu/Envs/nlp_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0024  0.0018  0.0001  0.0045  0.0043  0.9869\n",
       "[torch.cuda.FloatTensor of size 1x6 (GPU 0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=model(sample_phrase)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MultiLabelMarginLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sigh i see domer and big dunc are doing their tag thing routine again i dont know how you have the energy to persevere here your a better man than i',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('april 2010 please stop if you continue to add promotional material to wikipedia you will be blocked from editing talk',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('we also have this link it explains the negative stance on cair by investors weekly maybe its useful',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('no because i realized that adding tables to every section is sort of cruft mayby if we moved the fcw roster somewhere else then the page would be smaller and mayby one big table could be work also citations are just needed for their job the stuff written in italics by each superstar the roster pages cover really who is on what brand co',\n",
       "  (0, 0, 0, 0, 0, 0)),\n",
       " ('it wasnt me that edited those pages httpurdirtcom20091001ufc108silvavsbelfortrumors its ufc 108 silva vs belfort until further notice now please leave it alone and have a nice day',\n",
       "  (0, 0, 0, 0, 0, 0))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76680"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n_iters = 1000000\n",
    "num_epochs = n_iters/(len(x_train))/batch_size\n",
    "num_epochs=int(num_epochs)\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking incorrect predictions\n",
    "incorrect = 0\n",
    "incorrect_vect = pd.DataFrame(columns={'wrong_pred_intent', 'correct_intent','usersays'})\n",
    "incorrect_vect=incorrect_vect[['wrong_pred_intent', 'correct_intent','usersays']]\n",
    "incorrect_vect\n",
    "incorrect = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Envs/nlp_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "/home/ubuntu/Envs/nlp_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 1000. Loss: 4.36184047864982e-13\n",
      "Iterations: 2000. Loss: 2.6666667461395264\n",
      "Iterations: 3000. Loss: 1.1798955202102661\n",
      "Iterations: 4000. Loss: 0.0\n",
      "Iterations: 5000. Loss: 0.0\n",
      "Iterations: 6000. Loss: 0.0\n",
      "Iterations: 7000. Loss: 0.0\n",
      "Iterations: 8000. Loss: 0.0\n",
      "Iterations: 9000. Loss: 0.0\n",
      "Iterations: 10000. Loss: 0.00011645290214801207\n",
      "Iterations: 11000. Loss: 0.0\n",
      "Iterations: 12000. Loss: 5.54816580784719e-36\n",
      "Iterations: 13000. Loss: 1.2364866262310869e-28\n",
      "Iterations: 14000. Loss: 0.0\n",
      "Iterations: 15000. Loss: 2.0\n",
      "Iterations: 16000. Loss: 0.0\n",
      "Iterations: 17000. Loss: 0.0\n",
      "Iterations: 18000. Loss: 7.145776521610969e-07\n",
      "Iterations: 19000. Loss: 2.646989615343043e-13\n",
      "Iterations: 20000. Loss: 0.0\n",
      "Iterations: 21000. Loss: 1.463844019981491e-39\n",
      "Iterations: 22000. Loss: 0.0\n",
      "Iterations: 23000. Loss: 0.0\n",
      "Iterations: 24000. Loss: 0.0\n",
      "Iterations: 25000. Loss: 5.426860050207392e-23\n",
      "Iterations: 26000. Loss: 0.0\n",
      "Iterations: 27000. Loss: 0.0\n",
      "Iterations: 28000. Loss: 0.0\n",
      "Iterations: 29000. Loss: 0.0\n",
      "Iterations: 30000. Loss: 0.0\n",
      "Iterations: 31000. Loss: 0.0\n",
      "Iterations: 32000. Loss: 0.0\n",
      "Iterations: 33000. Loss: 0.0\n",
      "Iterations: 34000. Loss: 1.6097875638228698e-19\n",
      "Iterations: 35000. Loss: 0.0\n",
      "Iterations: 36000. Loss: 0.0\n",
      "Iterations: 37000. Loss: 0.0\n",
      "Iterations: 38000. Loss: 0.0\n",
      "Iterations: 39000. Loss: 0.0\n",
      "Iterations: 40000. Loss: 6.84766942742516e-19\n",
      "Iterations: 41000. Loss: 0.0\n",
      "Iterations: 42000. Loss: 0.0\n",
      "Iterations: 43000. Loss: 0.0\n",
      "Iterations: 44000. Loss: 0.0\n",
      "Iterations: 45000. Loss: 0.0\n",
      "Iterations: 46000. Loss: 0.0\n",
      "Iterations: 47000. Loss: 0.0\n",
      "Iterations: 48000. Loss: 8.688050478813866e-44\n",
      "Iterations: 49000. Loss: 0.0\n",
      "Iterations: 50000. Loss: 0.6666666865348816\n",
      "Iterations: 51000. Loss: 1.3280105546406291e-41\n",
      "Iterations: 52000. Loss: 0.0\n",
      "Iterations: 53000. Loss: 0.0\n",
      "Iterations: 54000. Loss: 0.0\n",
      "Iterations: 55000. Loss: 4.063195263059994e-27\n",
      "Iterations: 56000. Loss: 0.0\n",
      "Iterations: 57000. Loss: 0.0\n",
      "Iterations: 58000. Loss: 0.0\n",
      "Iterations: 59000. Loss: 2.2641653776547346e-23\n",
      "Iterations: 60000. Loss: 0.0\n",
      "Iterations: 61000. Loss: 1.5414283107572988e-43\n",
      "Iterations: 62000. Loss: 9.830704025731167e-35\n",
      "Iterations: 63000. Loss: 0.0\n",
      "Iterations: 64000. Loss: 1.3953960773710092e-31\n",
      "Iterations: 65000. Loss: 0.0\n",
      "Iterations: 66000. Loss: 6.467884759721113e-38\n",
      "Iterations: 67000. Loss: 1.5870869452416779e-25\n",
      "Iterations: 68000. Loss: 0.0\n",
      "Iterations: 69000. Loss: 1.3333333730697632\n",
      "Iterations: 70000. Loss: 8.76759482719003e-31\n",
      "Iterations: 71000. Loss: 0.0\n",
      "Iterations: 72000. Loss: 0.0\n",
      "Iterations: 73000. Loss: 0.0\n",
      "Iterations: 74000. Loss: 7.591669690581425e-22\n",
      "Iterations: 75000. Loss: 0.0\n",
      "Iterations: 76000. Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for (sent,label) in train_data:\n",
    "        # Step 1 - clear the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        ## Step 2- Prepare input and label\n",
    "        sample = Variable(make_sentence_embeddings(sent,word_to_ix)).cuda()\n",
    "        target = Variable(make_target(label)).cuda()\n",
    "    \n",
    "        # Step 3 - Run forward pass\n",
    "        output = model(sample)\n",
    "        #print(\"Log probabilities - {}\".format(log_probs))\n",
    "        \n",
    "        # Step 4 - Compute loss, gradients, update parameters\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter+=1      \n",
    "        ## Calculate final accuracy\n",
    "        if iter % 1000 ==0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (sent,label) in valid_data:\n",
    "                sample = Variable(make_sentence_embeddings(sent,word_to_ix)).cuda()\n",
    "                target = Variable(make_target(label)).cuda()\n",
    "                output = model(sample)\n",
    "                #print(output)\n",
    "                #_,predicted = torch.max(output.data,1)\n",
    "                #pred_label=list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted[0])]\n",
    "                #total += target.size(0)\n",
    "                #if (pred_label == label):\n",
    "                #    correct += (predicted[0] == make_target(label, label_to_ix)).sum()\n",
    "                #else:\n",
    "                #    incorrect_vect.loc[incorrect] = [pred_label, label, sent]\n",
    "                #    incorrect +=1\n",
    "            #accuracy = 100 * correct/total\n",
    "            print('Iterations: {}. Loss: {}'.format(iter,loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Envs/nlp_pytorch/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type BoWClassifierEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,'model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelito = torch.load('model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(model,test):\n",
    "    my_sub = pd.DataFrame(columns={'id', 'toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'})\n",
    "    my_sub=my_sub[['id', 'toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "    for i in range(len(test['cleaned_comment'])):\n",
    "        sample=test['cleaned_comment'][i]\n",
    "        if (sample != \"\"):\n",
    "            sample_context=Variable(make_sentence_embeddings(sample,word_to_ix)).cuda()\n",
    "            out=model(sample_context).data.cpu().numpy()[0]\n",
    "            #print(\"{} - {}\".format(test.loc[i]['id'],out))\n",
    "        else:\n",
    "            out = [1, 0, 0, 0, 0, 0]\n",
    "        my_sub.loc[i] = [test.loc[i]['id'], out[0], out[1], out[2], out[3], out[4], out[5]]\n",
    "    return my_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = make_preds(model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
