{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c EAP   HPL   MWS   \n",
      "l 35371 30273 27819 \n",
      "æ 36 10 0 \n",
      "R 258 237 385 \n",
      "δ 0 2 0 \n",
      "Σ 0 1 0 \n",
      "g 16088 14951 12601 \n",
      "k 4277 5204 3707 \n",
      "' 1334 1710 476 \n",
      "K 86 176 35 \n",
      "T 2217 1583 1230 \n",
      "N 411 345 204 \n",
      "C 395 439 308 \n",
      "r 51221 40590 44042 \n",
      "j 683 424 682 \n",
      "Υ 0 1 0 \n",
      "f 22354 16272 18351 \n",
      "â 6 0 0 \n",
      "Π 0 1 0 \n",
      "ö 16 3 0 \n",
      "ê 28 2 0 \n",
      "? 510 169 419 \n",
      "B 835 533 395 \n",
      "ô 8 0 0 \n",
      "Ν 0 1 0 \n",
      "è 15 0 0 \n",
      ". 8406 5908 5761 \n",
      "ü 1 5 0 \n",
      "Y 282 111 234 \n",
      "W 739 732 681 \n",
      "q 1030 779 677 \n",
      "F 383 269 232 \n",
      "Ο 0 3 0 \n",
      "O 414 503 282 \n",
      "L 458 249 307 \n",
      "V 156 67 57 \n",
      "d 36862 33366 35315 \n",
      "ä 1 6 0 \n",
      ": 176 47 339 \n",
      "h 51580 42770 43738 \n",
      "ñ 0 7 0 \n",
      "P 442 320 365 \n",
      "I 4846 3480 4917 \n",
      "w 17507 15554 16062 \n",
      "p 17422 10965 12361 \n",
      "c 24127 18338 17911 \n",
      "ἶ 0 2 0 \n",
      "o 67145 50996 53386 \n",
      "m 22792 17622 20471 \n",
      ", 17594 8581 12045 \n",
      "a 68525 56815 55274 \n",
      "v 9624 6529 7948 \n",
      "\" 2987 513 1469 \n",
      "à 10 0 0 \n",
      "u 26311 19519 21025 \n",
      "Q 21 10 7 \n",
      "s 53841 43915 45962 \n",
      "î 1 0 0 \n",
      "z 634 529 400 \n",
      "é 47 15 0 \n",
      "J 164 210 66 \n",
      "ë 0 12 0 \n",
      "Æ 1 4 0 \n",
      "ç 1 0 0 \n",
      "U 166 94 46 \n",
      "i 60952 44250 46080 \n",
      "n 62636 50879 50291 \n",
      "H 864 741 669 \n",
      "e 114885 88259 97515 \n",
      "ï 0 7 0 \n",
      "D 491 334 227 \n",
      "Å 0 1 0 \n",
      "; 1354 1143 2662 \n",
      "x 1951 1061 1267 \n",
      "E 435 281 445 \n",
      "A 1258 1167 943 \n",
      "G 313 318 246 \n",
      "S 729 841 578 \n",
      "M 1065 645 415 \n",
      "b 13245 10636 9611 \n",
      "α 0 2 0 \n",
      "Z 23 51 2 \n",
      "y 17001 12534 14877 \n",
      "X 17 5 4 \n",
      "t 82426 62235 63142 \n"
     ]
    }
   ],
   "source": [
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "\n",
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EAP', 'HPL', 'MWS']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    #text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Stemming\n",
    "    #text = gensim.parsing.preprocessing.stem_text(text)\n",
    "    #filtered_words = [word for word in text.split() if word not in stops]\n",
    "    #text = \" \".join(filtered_words)\n",
    "    \n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "     \n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process , however , afforded me no means of ascertaining the dimensions of my dungeon ; as I might make its circuit , and return to the point whence I set out , without being aware of the fact ; so perfectly uniform seemed the wall . This--process process--, ,--however however--, ,--afforded afforded--me me--no no--means means--of of--ascertaining ascertaining--the the--dimensions dimensions--of of--my my--dungeon dungeon--; ;--as as--I I--might might--make make--its its--circuit circuit--, ,--and and--return return--to to--the the--point point--whence whence--I I--set set--out out--, ,--without without--being being--aware aware--of of--the the--fact fact--; ;--so so--perfectly perfectly--uniform uniform--seemed seemed--the the--wall wall--.\n",
      "--------------------\n",
      "[174, 6008, 1, 224, 1, 2481, 26, 46, 469, 3, 20045, 2, 4827, 3, 15, 10367, 14, 21, 7, 120, 282, 59, 9408, 1, 5, 482, 6, 2, 393, 4601, 7, 533, 106, 1, 206, 182, 1587, 3, 2, 506, 14, 49, 2645, 11508, 142, 2, 725, 4, 20046, 245, 273, 45016, 9409, 4206, 1866, 1312, 31891, 31892, 90, 31893, 4602, 219, 704, 16908, 20047, 10, 16909, 1792, 42, 3239, 24545, 20048, 11509, 2231, 1046, 11510, 3666, 13, 2368, 1313, 31894, 20049, 1907, 5078]\n",
      "--------------------\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0   174  6008     1   224     1  2481    26    46\n",
      "   469     3 20045     2  4827     3    15 10367    14    21     7   120\n",
      "   282    59  9408     1     5   482     6     2   393  4601     7   533\n",
      "   106     1   206   182  1587     3     2   506    14    49  2645 11508\n",
      "   142     2   725     4 20046   245   273 45016  9409  4206  1866  1312\n",
      " 31891 31892    90 31893  4602   219   704 16908 20047    10 16909  1792\n",
      "    42  3239 24545 20048 11509  2231  1046 11510  3666    13  2368  1313\n",
      " 31894 20049  1907  5078]\n",
      "--------------------\n",
      "# words on training set: 76597\n"
     ]
    }
   ],
   "source": [
    "min_count = 2\n",
    "\n",
    "docs = create_docs(df)\n",
    "print(docs[0])\n",
    "print('-'*20)\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "print(docs[0])\n",
    "print('-'*20)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "print(docs[0])\n",
    "print('-'*20)\n",
    "print(\"# words on training set: {}\".format(num_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 256)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=embedding_dims, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model2(embedding_dims=embedding_dims, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "    model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "    \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/50\n",
      "15663/15663 [==============================] - 9s 572us/step - loss: 1.0764 - acc: 0.4071 - val_loss: 1.0596 - val_acc: 0.4109\n",
      "Epoch 2/50\n",
      "15663/15663 [==============================] - 9s 560us/step - loss: 1.0107 - acc: 0.4813 - val_loss: 0.9728 - val_acc: 0.5286\n",
      "Epoch 3/50\n",
      "15663/15663 [==============================] - 9s 557us/step - loss: 0.8842 - acc: 0.6752 - val_loss: 0.8524 - val_acc: 0.7314\n",
      "Epoch 4/50\n",
      "15663/15663 [==============================] - 9s 560us/step - loss: 0.7472 - acc: 0.7863 - val_loss: 0.7447 - val_acc: 0.7487\n",
      "Epoch 5/50\n",
      "15663/15663 [==============================] - 9s 558us/step - loss: 0.6311 - acc: 0.8320 - val_loss: 0.6635 - val_acc: 0.7865\n",
      "Epoch 6/50\n",
      "15663/15663 [==============================] - 9s 558us/step - loss: 0.5380 - acc: 0.8616 - val_loss: 0.6016 - val_acc: 0.8098\n",
      "Epoch 7/50\n",
      "15663/15663 [==============================] - 9s 586us/step - loss: 0.4623 - acc: 0.8871 - val_loss: 0.5535 - val_acc: 0.8121\n",
      "Epoch 8/50\n",
      "15663/15663 [==============================] - 9s 558us/step - loss: 0.4003 - acc: 0.9058 - val_loss: 0.5179 - val_acc: 0.8113\n",
      "Epoch 9/50\n",
      "15663/15663 [==============================] - 9s 557us/step - loss: 0.3480 - acc: 0.9210 - val_loss: 0.4831 - val_acc: 0.8338\n",
      "Epoch 10/50\n",
      "15663/15663 [==============================] - 9s 560us/step - loss: 0.3033 - acc: 0.9335 - val_loss: 0.4532 - val_acc: 0.8427\n",
      "Epoch 11/50\n",
      "15663/15663 [==============================] - 9s 567us/step - loss: 0.2650 - acc: 0.9432 - val_loss: 0.4309 - val_acc: 0.8486\n",
      "Epoch 12/50\n",
      "15663/15663 [==============================] - 9s 557us/step - loss: 0.2318 - acc: 0.9542 - val_loss: 0.4124 - val_acc: 0.8570\n",
      "Epoch 13/50\n",
      "15663/15663 [==============================] - 9s 561us/step - loss: 0.2028 - acc: 0.9623 - val_loss: 0.3998 - val_acc: 0.8565\n",
      "Epoch 14/50\n",
      "15663/15663 [==============================] - 9s 561us/step - loss: 0.1777 - acc: 0.9686 - val_loss: 0.3834 - val_acc: 0.8639\n",
      "Epoch 15/50\n",
      "15663/15663 [==============================] - 9s 559us/step - loss: 0.1559 - acc: 0.9741 - val_loss: 0.3744 - val_acc: 0.8631\n",
      "Epoch 16/50\n",
      "15663/15663 [==============================] - 9s 561us/step - loss: 0.1373 - acc: 0.9777 - val_loss: 0.3657 - val_acc: 0.8647\n",
      "Epoch 17/50\n",
      "15663/15663 [==============================] - 9s 557us/step - loss: 0.1203 - acc: 0.9814 - val_loss: 0.3558 - val_acc: 0.8687\n",
      "Epoch 18/50\n",
      "15663/15663 [==============================] - 9s 564us/step - loss: 0.1054 - acc: 0.9840 - val_loss: 0.3536 - val_acc: 0.8652\n",
      "Epoch 19/50\n",
      "15663/15663 [==============================] - 9s 562us/step - loss: 0.0923 - acc: 0.9871 - val_loss: 0.3462 - val_acc: 0.8700\n",
      "Epoch 20/50\n",
      "15663/15663 [==============================] - 9s 568us/step - loss: 0.0812 - acc: 0.9895 - val_loss: 0.3430 - val_acc: 0.8698\n",
      "Epoch 21/50\n",
      "15663/15663 [==============================] - 9s 556us/step - loss: 0.0717 - acc: 0.9906 - val_loss: 0.3419 - val_acc: 0.8685\n",
      "Epoch 22/50\n",
      "15663/15663 [==============================] - 9s 559us/step - loss: 0.0626 - acc: 0.9924 - val_loss: 0.3389 - val_acc: 0.8693\n",
      "Epoch 23/50\n",
      "15663/15663 [==============================] - 9s 554us/step - loss: 0.0554 - acc: 0.9932 - val_loss: 0.3428 - val_acc: 0.8685\n",
      "Epoch 24/50\n",
      "15663/15663 [==============================] - 9s 556us/step - loss: 0.0487 - acc: 0.9944 - val_loss: 0.3408 - val_acc: 0.8682\n",
      "Epoch 25/50\n",
      "15663/15663 [==============================] - 9s 557us/step - loss: 0.0429 - acc: 0.9956 - val_loss: 0.3370 - val_acc: 0.8698\n",
      "Epoch 26/50\n",
      "15663/15663 [==============================] - 9s 556us/step - loss: 0.0381 - acc: 0.9956 - val_loss: 0.3398 - val_acc: 0.8703\n",
      "Epoch 27/50\n",
      "15663/15663 [==============================] - 9s 562us/step - loss: 0.0335 - acc: 0.9966 - val_loss: 0.3405 - val_acc: 0.8685\n",
      "Epoch 28/50\n",
      "15663/15663 [==============================] - 9s 562us/step - loss: 0.0295 - acc: 0.9968 - val_loss: 0.3498 - val_acc: 0.8657\n",
      "Epoch 29/50\n",
      "15663/15663 [==============================] - 9s 563us/step - loss: 0.0261 - acc: 0.9971 - val_loss: 0.3473 - val_acc: 0.8677\n",
      "Epoch 30/50\n",
      "15663/15663 [==============================] - 9s 559us/step - loss: 0.0233 - acc: 0.9978 - val_loss: 0.3484 - val_acc: 0.8685\n",
      "Epoch 31/50\n",
      "15663/15663 [==============================] - 9s 559us/step - loss: 0.0207 - acc: 0.9981 - val_loss: 0.3544 - val_acc: 0.8670\n",
      "Epoch 32/50\n",
      "15663/15663 [==============================] - 9s 569us/step - loss: 0.0185 - acc: 0.9981 - val_loss: 0.3583 - val_acc: 0.8657\n",
      "Epoch 33/50\n",
      "15663/15663 [==============================] - 9s 562us/step - loss: 0.0163 - acc: 0.9984 - val_loss: 0.3622 - val_acc: 0.8677\n",
      "Epoch 34/50\n",
      "15663/15663 [==============================] - 9s 583us/step - loss: 0.0148 - acc: 0.9985 - val_loss: 0.3663 - val_acc: 0.8680\n",
      "Epoch 35/50\n",
      "15663/15663 [==============================] - 9s 573us/step - loss: 0.0130 - acc: 0.9989 - val_loss: 0.3708 - val_acc: 0.8657\n",
      "Epoch 36/50\n",
      "15663/15663 [==============================] - 9s 556us/step - loss: 0.0116 - acc: 0.9990 - val_loss: 0.3934 - val_acc: 0.8557\n",
      "Epoch 37/50\n",
      "15663/15663 [==============================] - 9s 562us/step - loss: 0.0104 - acc: 0.9990 - val_loss: 0.3889 - val_acc: 0.8647\n",
      "Epoch 38/50\n",
      "15663/15663 [==============================] - 9s 562us/step - loss: 0.0091 - acc: 0.9992 - val_loss: 0.3881 - val_acc: 0.8664\n",
      "Epoch 39/50\n",
      "15663/15663 [==============================] - 9s 554us/step - loss: 0.0084 - acc: 0.9991 - val_loss: 0.3953 - val_acc: 0.8647\n",
      "Epoch 40/50\n",
      "15663/15663 [==============================] - 9s 560us/step - loss: 0.0075 - acc: 0.9993 - val_loss: 0.4083 - val_acc: 0.8585\n",
      "Epoch 41/50\n",
      "15663/15663 [==============================] - 9s 555us/step - loss: 0.0069 - acc: 0.9993 - val_loss: 0.4059 - val_acc: 0.8631\n",
      "Epoch 42/50\n",
      "15663/15663 [==============================] - 9s 559us/step - loss: 0.0062 - acc: 0.9994 - val_loss: 0.4142 - val_acc: 0.8621\n",
      "Epoch 43/50\n",
      "15663/15663 [==============================] - 9s 558us/step - loss: 0.0055 - acc: 0.9995 - val_loss: 0.4314 - val_acc: 0.8618\n",
      "Epoch 44/50\n",
      "15663/15663 [==============================] - 9s 569us/step - loss: 0.0050 - acc: 0.9996 - val_loss: 0.4250 - val_acc: 0.8624\n",
      "Epoch 45/50\n",
      "15663/15663 [==============================] - 9s 575us/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.4340 - val_acc: 0.8621\n",
      "Epoch 46/50\n",
      "15663/15663 [==============================] - 9s 555us/step - loss: 0.0041 - acc: 0.9995 - val_loss: 0.4439 - val_acc: 0.8527\n",
      "Epoch 47/50\n",
      "15663/15663 [==============================] - 9s 555us/step - loss: 0.0037 - acc: 0.9996 - val_loss: 0.4493 - val_acc: 0.8555\n",
      "Epoch 48/50\n",
      "15663/15663 [==============================] - 9s 566us/step - loss: 0.0035 - acc: 0.9997 - val_loss: 0.4657 - val_acc: 0.8580\n",
      "Epoch 49/50\n",
      "15663/15663 [==============================] - 9s 562us/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.4763 - val_acc: 0.8590\n",
      "Epoch 50/50\n",
      "15663/15663 [==============================] - 9s 559us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.4689 - val_acc: 0.8573\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=32,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs)\n",
    "               #  ,callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss: 0.346\n"
     ]
    }
   ],
   "source": [
    "preds_proba = model.predict_proba(x_test)\n",
    "print(\"Log-loss: {0:.3f}\".format(log_loss(y_test, preds_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sub = pd.DataFrame(columns={'id', 'EAP','HPL', 'MWS'})\n",
    "my_sub=my_sub[['id', 'EAP','HPL', 'MWS']]\n",
    "my_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    my_sub.loc[i] = [test_df['id'][i], y[i][0], y[i][1], y[i][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sub.to_csv('roberto_new_keras_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
