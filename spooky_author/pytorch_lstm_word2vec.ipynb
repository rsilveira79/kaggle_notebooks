{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rsilveira79/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Plotting Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## NLP Libraries\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import download\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "spacy_en = spacy.load('en')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking if GPU is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'> - <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.cuda.FloatTensor([1])\n",
    "print(\"{} - {}\".format(type(a),type(a[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>id17718</td>\n",
       "      <td>I could have fancied, while I looked at it, th...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>id08973</td>\n",
       "      <td>The lids clenched themselves together as if in...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>id05267</td>\n",
       "      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>id17513</td>\n",
       "      <td>For an item of news like this, it strikes us i...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>id00393</td>\n",
       "      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author\n",
       "19574  id17718  I could have fancied, while I looked at it, th...    EAP\n",
       "19575  id08973  The lids clenched themselves together as if in...    EAP\n",
       "19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP\n",
       "19577  id17513  For an item of news like this, it strikes us i...    EAP\n",
       "19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "print(len(train))\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8387</th>\n",
       "      <td>id11749</td>\n",
       "      <td>All this is now the fitter for my purpose.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8388</th>\n",
       "      <td>id10526</td>\n",
       "      <td>I fixed myself on a wide solitude.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8389</th>\n",
       "      <td>id13477</td>\n",
       "      <td>It is easily understood that what might improv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8390</th>\n",
       "      <td>id13761</td>\n",
       "      <td>Be this as it may, I now began to feel the ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8391</th>\n",
       "      <td>id04282</td>\n",
       "      <td>Long winded, statistical, and drearily genealo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text\n",
       "8387  id11749         All this is now the fitter for my purpose.\n",
       "8388  id10526                 I fixed myself on a wide solitude.\n",
       "8389  id13477  It is easily understood that what might improv...\n",
       "8390  id13761  Be this as it may, I now began to feel the ins...\n",
       "8391  id04282  Long winded, statistical, and drearily genealo..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "print(len(test))\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Checking dataset unbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7900\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>19572</td>\n",
       "      <td>id03325</td>\n",
       "      <td>But these and other difficulties attending res...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7896</th>\n",
       "      <td>19574</td>\n",
       "      <td>id17718</td>\n",
       "      <td>I could have fancied, while I looked at it, th...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7897</th>\n",
       "      <td>19575</td>\n",
       "      <td>id08973</td>\n",
       "      <td>The lids clenched themselves together as if in...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7898</th>\n",
       "      <td>19576</td>\n",
       "      <td>id05267</td>\n",
       "      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7899</th>\n",
       "      <td>19577</td>\n",
       "      <td>id17513</td>\n",
       "      <td>For an item of news like this, it strikes us i...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       id                                               text author\n",
       "7895  19572  id03325  But these and other difficulties attending res...    EAP\n",
       "7896  19574  id17718  I could have fancied, while I looked at it, th...    EAP\n",
       "7897  19575  id08973  The lids clenched themselves together as if in...    EAP\n",
       "7898  19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP\n",
       "7899  19577  id17513  For an item of news like this, it strikes us i...    EAP"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EAP = train[train['author']=='EAP'].reset_index()\n",
    "EAP_size = len(EAP)\n",
    "print(EAP_size)\n",
    "EAP.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5635\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>19554</td>\n",
       "      <td>id07976</td>\n",
       "      <td>They admitted they had been drunk, but both vo...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>19559</td>\n",
       "      <td>id18823</td>\n",
       "      <td>When a fumbling came in the nearer casements h...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>19561</td>\n",
       "      <td>id08678</td>\n",
       "      <td>Average people in society and business New Eng...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td>19571</td>\n",
       "      <td>id14420</td>\n",
       "      <td>My watch was still going, and told me that the...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634</th>\n",
       "      <td>19578</td>\n",
       "      <td>id00393</td>\n",
       "      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       id                                               text author\n",
       "5630  19554  id07976  They admitted they had been drunk, but both vo...    HPL\n",
       "5631  19559  id18823  When a fumbling came in the nearer casements h...    HPL\n",
       "5632  19561  id08678  Average people in society and business New Eng...    HPL\n",
       "5633  19571  id14420  My watch was still going, and told me that the...    HPL\n",
       "5634  19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPL = train[train['author']=='HPL'].reset_index()\n",
    "HPL_size = len(HPL)\n",
    "print(HPL_size)\n",
    "HPL.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6044\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>19563</td>\n",
       "      <td>id10563</td>\n",
       "      <td>Yet from whom has not that rude hand rent away...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6040</th>\n",
       "      <td>19566</td>\n",
       "      <td>id00832</td>\n",
       "      <td>These reflections made our legislators pause, ...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>19569</td>\n",
       "      <td>id26790</td>\n",
       "      <td>Once my fancy was soothed with dreams of virtu...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>19570</td>\n",
       "      <td>id14263</td>\n",
       "      <td>Nay, you may have met with another whom you ma...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>19573</td>\n",
       "      <td>id07567</td>\n",
       "      <td>Stress of weather drove us up the Adriatic Gul...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       id                                               text author\n",
       "6039  19563  id10563  Yet from whom has not that rude hand rent away...    MWS\n",
       "6040  19566  id00832  These reflections made our legislators pause, ...    MWS\n",
       "6041  19569  id26790  Once my fancy was soothed with dreams of virtu...    MWS\n",
       "6042  19570  id14263  Nay, you may have met with another whom you ma...    MWS\n",
       "6043  19573  id07567  Stress of weather drove us up the Adriatic Gul...    MWS"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MWS = train[train['author']=='MWS'].reset_index()\n",
    "MWS_size = len(MWS)\n",
    "print(MWS_size)\n",
    "MWS.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>13951</td>\n",
       "      <td>id16972</td>\n",
       "      <td>I saw them now even more unequivocally than I ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>13952</td>\n",
       "      <td>id05477</td>\n",
       "      <td>For some time his countenance had been losing ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>13954</td>\n",
       "      <td>id17337</td>\n",
       "      <td>\"It is done it is most cheerfully agreed.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td>13956</td>\n",
       "      <td>id05256</td>\n",
       "      <td>\"At this stage of my reflections I endeavored ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634</th>\n",
       "      <td>13959</td>\n",
       "      <td>id12599</td>\n",
       "      <td>Coincidences ten times as remarkable as this t...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       id                                               text author\n",
       "5630  13951  id16972  I saw them now even more unequivocally than I ...    EAP\n",
       "5631  13952  id05477  For some time his countenance had been losing ...    EAP\n",
       "5632  13954  id17337          \"It is done it is most cheerfully agreed.    EAP\n",
       "5633  13956  id05256  \"At this stage of my reflections I endeavored ...    EAP\n",
       "5634  13959  id12599  Coincidences ten times as remarkable as this t...    EAP"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EAP[0:HPL_size].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16900</th>\n",
       "      <td>id22081</td>\n",
       "      <td>Wilson, this is your property.\"</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16901</th>\n",
       "      <td>id05401</td>\n",
       "      <td>At length there seemed to pass a violent and s...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16902</th>\n",
       "      <td>id10807</td>\n",
       "      <td>He endeavoured to guide with prudence the stee...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16903</th>\n",
       "      <td>id18870</td>\n",
       "      <td>The readiness with which I fell into a plan of...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16904</th>\n",
       "      <td>id09898</td>\n",
       "      <td>And where Nyarlathotep went, rest vanished; fo...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author\n",
       "16900  id22081                    Wilson, this is your property.\"    EAP\n",
       "16901  id05401  At length there seemed to pass a violent and s...    EAP\n",
       "16902  id10807  He endeavoured to guide with prudence the stee...    MWS\n",
       "16903  id18870  The readiness with which I fell into a plan of...    HPL\n",
       "16904  id09898  And where Nyarlathotep went, rest vanished; fo...    HPL"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_undersampled = pd.concat([EAP[0:HPL_size],HPL,MWS[0:HPL_size]], ignore_index=True)\n",
    "train_undersampled.drop(['index'],axis=1,inplace=True)\n",
    "train_undersampled = train_undersampled.sample(frac=1).reset_index(drop=True)\n",
    "train_undersampled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19579"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformText(text, do_stop=False, do_stem=False):\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    \n",
    "    if (do_stop==True):\n",
    "        filtered_words = [word for word in text.split() if word not in stops]\n",
    "    else:\n",
    "        filtered_words = [word for word in text.split()]\n",
    "\n",
    "    # Removing all the tokens with lesser than 3 characters\n",
    "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=2)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    if (do_stem==True):\n",
    "        # Stemming\n",
    "        text = gensim.parsing.preprocessing.stem_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>phrase_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id21564</td>\n",
       "      <td>Now I am the first man in the state, burthen o...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>now am the first man in the state burthen of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id00662</td>\n",
       "      <td>But for one thing Old Bugs would have been an ...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>but for one thing old bugs would have been an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id03160</td>\n",
       "      <td>His habits were unhinged; his restless mind ro...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>his habits were unhinged his restless mind rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id15401</td>\n",
       "      <td>These Great Old Ones, Castro continued, were n...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>these great old ones castro continued were not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id00866</td>\n",
       "      <td>I guess I done a bit by tellin' Selectman Mowr...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>guess done bit by tellin selectman mowry what ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id21564  Now I am the first man in the state, burthen o...    MWS   \n",
       "1  id00662  But for one thing Old Bugs would have been an ...    HPL   \n",
       "2  id03160  His habits were unhinged; his restless mind ro...    MWS   \n",
       "3  id15401  These Great Old Ones, Castro continued, were n...    HPL   \n",
       "4  id00866  I guess I done a bit by tellin' Selectman Mowr...    HPL   \n",
       "\n",
       "                                 phrase_preprocessed  \n",
       "0  now am the first man in the state burthen of e...  \n",
       "1  but for one thing old bugs would have been an ...  \n",
       "2  his habits were unhinged his restless mind rou...  \n",
       "3  these great old ones castro continued were not...  \n",
       "4  guess done bit by tellin selectman mowry what ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_undersampled['phrase_preprocessed']=train_undersampled['text'].apply(lambda x: transformText(x,do_stop=False, do_stem=False))\n",
    "train_undersampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>phrase_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>still as urged our leaving ireland with such i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>if fire wanted fanning it could readily be fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>and when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>while was thinking how should possibly manage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>am not sure to what limit his knowledge may ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "3  id27757  While I was thinking how I should possibly man...   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...   \n",
       "\n",
       "                                 phrase_preprocessed  \n",
       "0  still as urged our leaving ireland with such i...  \n",
       "1  if fire wanted fanning it could readily be fan...  \n",
       "2  and when they had broken down the frail door t...  \n",
       "3  while was thinking how should possibly manage ...  \n",
       "4  am not sure to what limit his knowledge may ex...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['phrase_preprocessed']=test['text'].apply(lambda x: transformText(x,do_stop=False, do_stem=False))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test split, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_undersampled['phrase_preprocessed'],\n",
    "                                                      train_undersampled['author'], \n",
    "                                                      test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'still as urged our leaving ireland with such inquietude and impatience my father thought it best to yield ',\n",
       "       'if fire wanted fanning it could readily be fanned with newspaper and as the government grew weaker have no doubt that leather and iron acquired durability in proportion for in very short time there was not pair of bellows in all rotterdam that ever stood in need of stitch or required the assistance of hammer ',\n",
       "       'and when they had broken down the frail door they found only this two cleanly picked human skeletons on the earthen floor and number of singular beetles crawling in the shadowy corners ',\n",
       "       ...,\n",
       "       'it is easily understood that what might improve closely scrutinized detail may at the same time injure general or more distantly observed effect ',\n",
       "       'be this as it may now began to feel the inspiration of burning hope and at length nurtured in my secret thoughts stern and desperate resolution that would submit no longer to be enslaved ',\n",
       "       'long winded statistical and drearily genealogical as some of the matter was there ran through it continuous thread of brooding tenacious horror and preternatural malevolence which impressed me even more than it had impressed the good doctor '], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.array(test['phrase_preprocessed'])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Build Vocabulary\n",
    "word_to_ix = {}\n",
    "for sent in list(x_train) + list(x_valid) + list(x_test):\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 27369\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(word_to_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_ix = { \"EAP\": 0, \"MWS\": 1, \"HPL\": 2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27369, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = len(label_to_ix)\n",
    "VOCAB_SIZE, NUM_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the merriment of the hour was an unholy mockery of the sorrows of man ',\n",
       "  'MWS'),\n",
       " ('but the language of the evidence speaks of the strip in question as found around the neck fitting loosely and secured with hard knot ',\n",
       "  'EAP'),\n",
       " ('he could he was sure get out by midnight though it is characteristic of him that this thought was untinged with eerie implications ',\n",
       "  'HPL'),\n",
       " ('in the meantime the lunatics had jolly season of it that you may swear ',\n",
       "  'EAP'),\n",
       " ('klenze though not my mental equal was much better than no one ', 'HPL')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=list(zip(x_train,y_train))\n",
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the true wretchedness indeed the ultimate woe is particular not diffuse ',\n",
       "  'EAP'),\n",
       " ('if the study to which you apply yourself has tendency to weaken your affections and to destroy your taste for those simple pleasures in which no alloy can possibly mix then that study is certainly unlawful that is to say not befitting the human mind ',\n",
       "  'MWS'),\n",
       " ('that glimpse like all dread glimpses of truth flashed out from an accidental piecing together of separated things in this case an old newspaper item and the notes of dead professor ',\n",
       "  'HPL'),\n",
       " ('then as remained paralysed with fear he found his voice and in his dying breath screamed forth those words which have ever afterward haunted my days and my nights ',\n",
       "  'HPL'),\n",
       " ('observed however that the shutters of the fourth story were of the peculiar kind called by parisian carpenters ferrades kind rarely employed at the present day but frequently seen upon very old mansions at lyons and bordeaux ',\n",
       "  'EAP')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data=list(zip(x_valid,y_valid))\n",
    "valid_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_context_vector(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq.split()]\n",
    "    tensor = torch.cuda.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_target(label, label_to_idx):\n",
    "    return torch.cuda.LongTensor([label_to_idx[label]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model - LSTM Classifier with Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.42B.300d.txt\t\t       wget-log.1\r\n",
      "GoogleNews-vectors-negative300.bin.gz  wiki-news-300d-1M-subword.vec\r\n",
      "wget-log\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../vectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_path = '../../vectors/glove.42B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1917494  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove_vector = loadGloveModel(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3013  ,  0.49315 , -0.47456 , -0.55259 , -0.079275, -0.069589,\n",
       "       -3.5792  ,  0.28671 , -0.034099, -0.23643 , -0.042276,  0.25939 ,\n",
       "       -0.080374, -0.25137 ,  0.22347 ,  0.22123 ,  0.077188,  0.41157 ,\n",
       "       -0.1154  , -0.065081, -0.1549  , -0.08466 ,  0.078873, -0.109   ,\n",
       "       -0.018008,  0.2591  ,  0.09544 , -0.33333 , -0.24942 , -0.29616 ,\n",
       "       -0.60454 , -0.11145 ,  0.21661 , -0.17045 ,  0.2329  ,  0.24699 ,\n",
       "       -0.37996 , -0.035888, -0.035235,  0.14275 , -0.13491 ,  0.018954,\n",
       "        0.35121 , -0.34182 ,  0.041529, -0.48883 ,  0.29649 ,  0.25711 ,\n",
       "        0.21757 , -0.04452 ,  0.49266 ,  0.30706 , -0.30375 ,  0.12427 ,\n",
       "       -0.090547,  0.055427, -0.357   ,  0.049935, -0.12842 ,  0.037939,\n",
       "        0.20706 ,  0.24645 ,  0.45067 ,  0.26154 , -0.39262 ,  0.10625 ,\n",
       "       -0.090539,  0.13802 ,  0.088777, -0.4219  , -0.14283 ,  0.053959,\n",
       "       -0.40817 , -0.13262 , -0.44289 ,  0.046249,  0.53221 ,  0.030767,\n",
       "        0.06197 , -0.18862 , -0.28016 ,  0.09877 ,  0.064807,  0.10248 ,\n",
       "        0.28618 ,  0.47056 ,  0.40463 , -0.1651  , -0.086233,  0.17781 ,\n",
       "       -0.28773 , -0.10748 ,  0.18487 , -0.054304,  0.21486 , -0.43958 ,\n",
       "       -1.9126  ,  0.27618 ,  0.60843 ,  0.38416 ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vector['start'][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#w2v = KeyedVectors.load_word2vec_format('../../vectors/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2V_DIM = 300\n",
    "## standard deviation to use\n",
    "sd = 1/np.sqrt(W2V_DIM)\n",
    "## Random initialization\n",
    "weights = np.random.normal(0, scale=sd, size=[VOCAB_SIZE, W2V_DIM])\n",
    "weights = weights.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in word_to_ix:\n",
    "    id = word_to_ix.get(word,None)\n",
    "    if id is not None:\n",
    "        try:\n",
    "            #weights[id]=w2v.wv.word_vec(word)\n",
    "            weights[id]=glove_vector[word]\n",
    "        except:\n",
    "            weights[id]=np.random.normal(0, scale=sd, size=[1, W2V_DIM]) ## If word not present, initialize randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'merriment': 1,\n",
       " 'of': 2,\n",
       " 'hour': 3,\n",
       " 'was': 4,\n",
       " 'an': 5,\n",
       " 'unholy': 6,\n",
       " 'mockery': 7,\n",
       " 'sorrows': 8,\n",
       " 'man': 9,\n",
       " 'but': 10,\n",
       " 'language': 11,\n",
       " 'evidence': 12,\n",
       " 'speaks': 13,\n",
       " 'strip': 14,\n",
       " 'in': 15,\n",
       " 'question': 16,\n",
       " 'as': 17,\n",
       " 'found': 18,\n",
       " 'around': 19,\n",
       " 'neck': 20,\n",
       " 'fitting': 21,\n",
       " 'loosely': 22,\n",
       " 'and': 23,\n",
       " 'secured': 24,\n",
       " 'with': 25,\n",
       " 'hard': 26,\n",
       " 'knot': 27,\n",
       " 'he': 28,\n",
       " 'could': 29,\n",
       " 'sure': 30,\n",
       " 'get': 31,\n",
       " 'out': 32,\n",
       " 'by': 33,\n",
       " 'midnight': 34,\n",
       " 'though': 35,\n",
       " 'it': 36,\n",
       " 'is': 37,\n",
       " 'characteristic': 38,\n",
       " 'him': 39,\n",
       " 'that': 40,\n",
       " 'this': 41,\n",
       " 'thought': 42,\n",
       " 'untinged': 43,\n",
       " 'eerie': 44,\n",
       " 'implications': 45,\n",
       " 'meantime': 46,\n",
       " 'lunatics': 47,\n",
       " 'had': 48,\n",
       " 'jolly': 49,\n",
       " 'season': 50,\n",
       " 'you': 51,\n",
       " 'may': 52,\n",
       " 'swear': 53,\n",
       " 'klenze': 54,\n",
       " 'not': 55,\n",
       " 'my': 56,\n",
       " 'mental': 57,\n",
       " 'equal': 58,\n",
       " 'much': 59,\n",
       " 'better': 60,\n",
       " 'than': 61,\n",
       " 'no': 62,\n",
       " 'one': 63,\n",
       " 'new': 64,\n",
       " 'system': 65,\n",
       " 'your': 66,\n",
       " 'own': 67,\n",
       " 'invention': 68,\n",
       " 'sunny': 69,\n",
       " 'weather': 70,\n",
       " 'glad': 71,\n",
       " 'bar': 72,\n",
       " 'now': 73,\n",
       " 'four': 74,\n",
       " 'inches': 75,\n",
       " 'half': 76,\n",
       " 'deep': 77,\n",
       " 'there': 78,\n",
       " 'only': 79,\n",
       " 'little': 80,\n",
       " 'bit': 81,\n",
       " 'skin': 82,\n",
       " 'to': 83,\n",
       " 'cut': 84,\n",
       " 'through': 85,\n",
       " 'senses': 86,\n",
       " 'were': 87,\n",
       " 'gratified': 88,\n",
       " 'refreshed': 89,\n",
       " 'thousand': 90,\n",
       " 'scents': 91,\n",
       " 'delight': 92,\n",
       " 'sights': 93,\n",
       " 'beauty': 94,\n",
       " 'nevertheless': 95,\n",
       " 'change': 96,\n",
       " 'menace': 97,\n",
       " 'from': 98,\n",
       " 'vague': 99,\n",
       " 'premonition': 100,\n",
       " 'immediate': 101,\n",
       " 'reality': 102,\n",
       " 'profound': 103,\n",
       " 'shock': 104,\n",
       " 'fell': 105,\n",
       " 'upon': 106,\n",
       " 'me': 107,\n",
       " 'force': 108,\n",
       " 'genuine': 109,\n",
       " 'blow': 110,\n",
       " 'came': 111,\n",
       " 'some': 112,\n",
       " 'gentle': 113,\n",
       " 'visitation': 114,\n",
       " 'went': 115,\n",
       " 'hardly': 116,\n",
       " 'felt': 117,\n",
       " 'loss': 118,\n",
       " 'manton': 119,\n",
       " 'remained': 120,\n",
       " 'thoughtful': 121,\n",
       " 'said': 122,\n",
       " 'gradually': 123,\n",
       " 'reverted': 124,\n",
       " 'his': 125,\n",
       " 'analytical': 126,\n",
       " 'mood': 127,\n",
       " 'they': 128,\n",
       " 'darted': 129,\n",
       " 'almost': 130,\n",
       " 'noiselessly': 131,\n",
       " 'underbrush': 132,\n",
       " 'astonished': 133,\n",
       " 'watcher': 134,\n",
       " 'seemed': 135,\n",
       " 'think': 136,\n",
       " 'entirely': 137,\n",
       " 'unclothed': 138,\n",
       " 'why': 139,\n",
       " 'cannot': 140,\n",
       " 'doctors': 141,\n",
       " 'give': 142,\n",
       " 'something': 143,\n",
       " 'make': 144,\n",
       " 'sleep': 145,\n",
       " 'or': 146,\n",
       " 'truly': 147,\n",
       " 'calm': 148,\n",
       " 'brain': 149,\n",
       " 'when': 150,\n",
       " 'thunders': 151,\n",
       " 'crown': 152,\n",
       " 'vexation': 153,\n",
       " 'spirit': 154,\n",
       " 'perdita': 155,\n",
       " 'visionary': 156,\n",
       " 'awake': 157,\n",
       " 'real': 158,\n",
       " 'life': 159,\n",
       " 'transport': 160,\n",
       " 'she': 161,\n",
       " 'told': 162,\n",
       " 'earl': 163,\n",
       " 'windsor': 164,\n",
       " 'about': 165,\n",
       " 'arrive': 166,\n",
       " 'departs': 167,\n",
       " 'shall': 168,\n",
       " 'child': 169,\n",
       " 'live': 170,\n",
       " 'thy': 171,\n",
       " 'mine': 172,\n",
       " 'morella': 173,\n",
       " 's': 174,\n",
       " 'ahead': 175,\n",
       " 'touched': 176,\n",
       " 'back': 177,\n",
       " 'zann': 178,\n",
       " 'chair': 179,\n",
       " 'then': 180,\n",
       " 'shook': 181,\n",
       " 'shoulder': 182,\n",
       " 'effort': 183,\n",
       " 'bring': 184,\n",
       " 'at': 185,\n",
       " 'end': 186,\n",
       " 'tenth': 187,\n",
       " 'day': 188,\n",
       " 'advisable': 189,\n",
       " 'double': 190,\n",
       " 'sum': 191,\n",
       " 'originally': 192,\n",
       " 'proposed': 193,\n",
       " 'length': 194,\n",
       " 'second': 195,\n",
       " 'week': 196,\n",
       " 'having': 197,\n",
       " 'elapsed': 198,\n",
       " 'without': 199,\n",
       " 'leading': 200,\n",
       " 'any': 201,\n",
       " 'discoveries': 202,\n",
       " 'prejudice': 203,\n",
       " 'which': 204,\n",
       " 'always': 205,\n",
       " 'exists': 206,\n",
       " 'paris': 207,\n",
       " 'against': 208,\n",
       " 'police': 209,\n",
       " 'given': 210,\n",
       " 'vent': 211,\n",
       " 'itself': 212,\n",
       " 'several': 213,\n",
       " 'serious': 214,\n",
       " 'meutes': 215,\n",
       " 'prefect': 216,\n",
       " 'took': 217,\n",
       " 'himself': 218,\n",
       " 'offer': 219,\n",
       " 'twenty': 220,\n",
       " 'francs': 221,\n",
       " 'for': 222,\n",
       " 'conviction': 223,\n",
       " 'assassin': 224,\n",
       " 'if': 225,\n",
       " 'more': 226,\n",
       " 'should': 227,\n",
       " 'prove': 228,\n",
       " 'have': 229,\n",
       " 'been': 230,\n",
       " 'implicated': 231,\n",
       " 'assassins': 232,\n",
       " 'became': 233,\n",
       " 'intimate': 234,\n",
       " 'each': 235,\n",
       " 'afforded': 236,\n",
       " 'occasion': 237,\n",
       " 'admire': 238,\n",
       " 'powerful': 239,\n",
       " 'versatile': 240,\n",
       " 'talents': 241,\n",
       " 'together': 242,\n",
       " 'eloquence': 243,\n",
       " 'graceful': 244,\n",
       " 'witty': 245,\n",
       " 'wealth': 246,\n",
       " 'immense': 247,\n",
       " 'caused': 248,\n",
       " 'be': 249,\n",
       " 'feared': 250,\n",
       " 'loved': 251,\n",
       " 'hated': 252,\n",
       " 'beyond': 253,\n",
       " 'other': 254,\n",
       " 'england': 255,\n",
       " 'after': 256,\n",
       " 'fading': 257,\n",
       " 'last': 258,\n",
       " 'match': 259,\n",
       " 'dared': 260,\n",
       " 'waste': 261,\n",
       " 'sat': 262,\n",
       " 'very': 263,\n",
       " 'quietly': 264,\n",
       " 'light': 265,\n",
       " 'propped': 266,\n",
       " 'level': 267,\n",
       " 'on': 268,\n",
       " 'floor': 269,\n",
       " 'low': 270,\n",
       " 'cases': 271,\n",
       " 'full': 272,\n",
       " 'books': 273,\n",
       " 'every': 274,\n",
       " 'degree': 275,\n",
       " 'antiquity': 276,\n",
       " 'disintegration': 277,\n",
       " 'centre': 278,\n",
       " 'table': 279,\n",
       " 'bench': 280,\n",
       " 'both': 281,\n",
       " 'apparently': 282,\n",
       " 'fastened': 283,\n",
       " 'place': 284,\n",
       " 'often': 285,\n",
       " 'refused': 286,\n",
       " 'accompany': 287,\n",
       " 'alleging': 288,\n",
       " 'another': 289,\n",
       " 'engagement': 290,\n",
       " 'might': 291,\n",
       " 'remain': 292,\n",
       " 'alone': 293,\n",
       " 'heard': 294,\n",
       " 'phial': 295,\n",
       " 'break': 296,\n",
       " 'harmlessly': 297,\n",
       " 'stones': 298,\n",
       " 'passage': 299,\n",
       " 'tunic': 300,\n",
       " 'strange': 301,\n",
       " 'caught': 302,\n",
       " 'fire': 303,\n",
       " 'lit': 304,\n",
       " 'horrid': 305,\n",
       " 'scene': 306,\n",
       " 'ghastly': 307,\n",
       " 'radiance': 308,\n",
       " 'cold': 309,\n",
       " 'blood': 310,\n",
       " 'heart': 311,\n",
       " 'sickened': 312,\n",
       " 'work': 313,\n",
       " 'hands': 314,\n",
       " 'voice': 315,\n",
       " 'englishman': 316,\n",
       " 'et': 317,\n",
       " 'chez': 318,\n",
       " 'j': 319,\n",
       " 'goignard': 320,\n",
       " 'au': 321,\n",
       " 'premier': 322,\n",
       " 'pilier': 323,\n",
       " 'de': 324,\n",
       " 'la': 325,\n",
       " 'grand': 326,\n",
       " 'salle': 327,\n",
       " 'du': 328,\n",
       " 'palais': 329,\n",
       " 'proche': 330,\n",
       " 'les': 331,\n",
       " 'consultations': 332,\n",
       " 'mdcxlvii': 333,\n",
       " 'request': 334,\n",
       " 'must': 335,\n",
       " 'confess': 336,\n",
       " 'maternal': 337,\n",
       " 'affection': 338,\n",
       " 'rendered': 339,\n",
       " 'idris': 340,\n",
       " 'selfish': 341,\n",
       " 'beginning': 342,\n",
       " 'our': 343,\n",
       " 'calamity': 344,\n",
       " 'thoughtless': 345,\n",
       " 'enthusiasm': 346,\n",
       " 'devoted': 347,\n",
       " 'herself': 348,\n",
       " 'care': 349,\n",
       " 'sick': 350,\n",
       " 'helpless': 351,\n",
       " 'whole': 352,\n",
       " 'series': 353,\n",
       " 'appeared': 354,\n",
       " 'dream': 355,\n",
       " 'sometimes': 356,\n",
       " 'doubted': 357,\n",
       " 'indeed': 358,\n",
       " 'all': 359,\n",
       " 'true': 360,\n",
       " 'never': 361,\n",
       " 'presented': 362,\n",
       " 'mind': 363,\n",
       " 'hoary': 364,\n",
       " 'nodens': 365,\n",
       " 'reached': 366,\n",
       " 'forth': 367,\n",
       " 'wizened': 368,\n",
       " 'hand': 369,\n",
       " 'helped': 370,\n",
       " 'olney': 371,\n",
       " 'host': 372,\n",
       " 'into': 373,\n",
       " 'vast': 374,\n",
       " 'shell': 375,\n",
       " 'whereat': 376,\n",
       " 'conches': 377,\n",
       " 'gongs': 378,\n",
       " 'set': 379,\n",
       " 'up': 380,\n",
       " 'wild': 381,\n",
       " 'awesome': 382,\n",
       " 'clamour': 383,\n",
       " 'part': 384,\n",
       " 'intend': 385,\n",
       " 'believe': 386,\n",
       " 'nothing': 387,\n",
       " 'henceforward': 388,\n",
       " 'has': 389,\n",
       " 'anything': 390,\n",
       " 'singular': 391,\n",
       " 'abaout': 392,\n",
       " 'knows': 393,\n",
       " 'kin': 394,\n",
       " 'tell': 395,\n",
       " 'ye': 396,\n",
       " 'mamie': 397,\n",
       " 'naowadays': 398,\n",
       " 'nor': 399,\n",
       " 'what': 400,\n",
       " 'know': 401,\n",
       " 'myself': 402,\n",
       " 'returned': 403,\n",
       " 'boat': 404,\n",
       " 'electric': 405,\n",
       " 'batteries': 406,\n",
       " 'grew': 407,\n",
       " 'feeble': 408,\n",
       " 'resolved': 409,\n",
       " 'explore': 410,\n",
       " 'rock': 411,\n",
       " 'temple': 412,\n",
       " 'following': 413,\n",
       " 'opposite': 414,\n",
       " 'base': 415,\n",
       " 'sentinel': 416,\n",
       " 'hill': 417,\n",
       " 'tracks': 418,\n",
       " 'left': 419,\n",
       " 'road': 420,\n",
       " 'fresh': 421,\n",
       " 'bending': 422,\n",
       " 'matting': 423,\n",
       " 'visible': 424,\n",
       " 'along': 425,\n",
       " 'broad': 426,\n",
       " 'swath': 427,\n",
       " 'marking': 428,\n",
       " 'monster': 429,\n",
       " 'former': 430,\n",
       " 'route': 431,\n",
       " 'summit': 432,\n",
       " 'employ': 433,\n",
       " 'year': 434,\n",
       " 'her': 435,\n",
       " 'admirers': 436,\n",
       " 'thrown': 437,\n",
       " 'info': 438,\n",
       " 'confusion': 439,\n",
       " 'sudden': 440,\n",
       " 'disappearance': 441,\n",
       " 'shop': 442,\n",
       " 'since': 443,\n",
       " 'object': 444,\n",
       " 'indifference': 445,\n",
       " 'contempt': 446,\n",
       " 'far': 447,\n",
       " 'avoid': 448,\n",
       " 'expose': 449,\n",
       " 'before': 450,\n",
       " 'scornful': 451,\n",
       " 'world': 452,\n",
       " 'chance': 453,\n",
       " 'playing': 454,\n",
       " 'mad': 455,\n",
       " 'game': 456,\n",
       " 'fond': 457,\n",
       " 'foolish': 458,\n",
       " 'icarus': 459,\n",
       " 'believed': 460,\n",
       " 'innocence': 461,\n",
       " 'knew': 462,\n",
       " 'next': 463,\n",
       " 'step': 464,\n",
       " 're': 465,\n",
       " 'examine': 466,\n",
       " 'microscopic': 467,\n",
       " 'deserted': 468,\n",
       " 'hamlet': 469,\n",
       " 'where': 470,\n",
       " 'death': 471,\n",
       " 'come': 472,\n",
       " 'most': 473,\n",
       " 'abundantly': 474,\n",
       " 'arthur': 475,\n",
       " 'munroe': 476,\n",
       " 'seen': 477,\n",
       " 'lived': 478,\n",
       " 'describe': 479,\n",
       " 'houses': 480,\n",
       " 'cities': 481,\n",
       " 'cathuria': 482,\n",
       " 'are': 483,\n",
       " 'palaces': 484,\n",
       " 'built': 485,\n",
       " 'over': 486,\n",
       " 'fragrant': 487,\n",
       " 'canal': 488,\n",
       " 'bearing': 489,\n",
       " 'waters': 490,\n",
       " 'sacred': 491,\n",
       " 'narg': 492,\n",
       " 'motion': 493,\n",
       " 'invariably': 494,\n",
       " 'precedes': 495,\n",
       " 'two': 496,\n",
       " 'seconds': 497,\n",
       " 'movement': 498,\n",
       " 'arm': 499,\n",
       " 'instance': 500,\n",
       " 'moves': 501,\n",
       " 'preparatory': 502,\n",
       " 'cabbages': 503,\n",
       " 'turned': 504,\n",
       " 'red': 505,\n",
       " 'face': 506,\n",
       " 'old': 507,\n",
       " 'nick': 508,\n",
       " 'taken': 509,\n",
       " 'possession': 510,\n",
       " 'thing': 511,\n",
       " 'shape': 512,\n",
       " 'timepiece': 513,\n",
       " 'tongues': 514,\n",
       " 'flame': 515,\n",
       " 'searing': 516,\n",
       " 'gusts': 517,\n",
       " 'heat': 518,\n",
       " 'engulfed': 519,\n",
       " 'house': 520,\n",
       " 'roysterers': 521,\n",
       " 'struck': 522,\n",
       " 'terror': 523,\n",
       " 'descent': 524,\n",
       " 'transcend': 525,\n",
       " 'bounds': 526,\n",
       " 'unguided': 527,\n",
       " 'nature': 528,\n",
       " 'fled': 529,\n",
       " 'shrieking': 530,\n",
       " 'night': 531,\n",
       " 'peace': 532,\n",
       " 'learn': 533,\n",
       " 'miseries': 534,\n",
       " 'do': 535,\n",
       " 'seek': 536,\n",
       " 'increase': 537,\n",
       " 'who': 538,\n",
       " 'did': 539,\n",
       " 'notwithstanding': 540,\n",
       " 'disappointment': 541,\n",
       " 'quelled': 542,\n",
       " 'passion': 543,\n",
       " 'ambition': 544,\n",
       " 'held': 545,\n",
       " 'strong': 546,\n",
       " 'combat': 547,\n",
       " 'these': 548,\n",
       " 'phenomena': 549,\n",
       " 'occasioned': 550,\n",
       " 'expansion': 551,\n",
       " 'gas': 552,\n",
       " 'atmosphere': 553,\n",
       " 'consequent': 554,\n",
       " 'disruption': 555,\n",
       " 'minute': 556,\n",
       " 'particles': 557,\n",
       " 'ice': 558,\n",
       " 'network': 559,\n",
       " 'become': 560,\n",
       " 'encrusted': 561,\n",
       " 'during': 562,\n",
       " 'immediately': 563,\n",
       " 'beneath': 564,\n",
       " 'ocean': 565,\n",
       " 'lay': 566,\n",
       " 'small': 567,\n",
       " 'black': 568,\n",
       " 'slightly': 569,\n",
       " 'oblong': 570,\n",
       " 'seemingly': 571,\n",
       " 'size': 572,\n",
       " 'way': 573,\n",
       " 'great': 574,\n",
       " 'resemblance': 575,\n",
       " 'those': 576,\n",
       " 'childish': 577,\n",
       " 'toys': 578,\n",
       " 'called': 579,\n",
       " 'domino': 580,\n",
       " 'inspired': 581,\n",
       " 'casimir': 582,\n",
       " 'perier': 583,\n",
       " 'whose': 584,\n",
       " 'pert': 585,\n",
       " 'query': 586,\n",
       " 'a': 587,\n",
       " 'quoi': 588,\n",
       " 'un': 589,\n",
       " 'poete': 590,\n",
       " 'est': 591,\n",
       " 'il': 592,\n",
       " 'bon': 593,\n",
       " 'habit': 594,\n",
       " 'quoting': 595,\n",
       " 'droll': 596,\n",
       " 'pronunciation': 597,\n",
       " 'ne': 598,\n",
       " 'plus': 599,\n",
       " 'ultra': 600,\n",
       " 'logical': 601,\n",
       " 'wit': 602,\n",
       " 'fearlessness': 603,\n",
       " 'frankness': 604,\n",
       " 'would': 605,\n",
       " 'encroach': 606,\n",
       " 'liberty': 607,\n",
       " 'unassailable': 608,\n",
       " 'so': 609,\n",
       " 'wholly': 610,\n",
       " 'uneducated': 611,\n",
       " 'silent': 612,\n",
       " 'turk': 613,\n",
       " 'kind': 614,\n",
       " 'ignorant': 615,\n",
       " 'carelessness': 616,\n",
       " 'attends': 617,\n",
       " 'while': 618,\n",
       " 'renders': 619,\n",
       " 'conduct': 620,\n",
       " 'astonishing': 621,\n",
       " 'detracts': 622,\n",
       " 'interest': 623,\n",
       " 'sympathy': 624,\n",
       " 'otherwise': 625,\n",
       " 'command': 626,\n",
       " 'case': 627,\n",
       " 'differed': 628,\n",
       " 'important': 629,\n",
       " 'particular': 630,\n",
       " 'mentioned': 631,\n",
       " 'medical': 632,\n",
       " 'land': 633,\n",
       " 'wanderers': 634,\n",
       " 'none': 635,\n",
       " 'prayers': 636,\n",
       " 'painted': 637,\n",
       " 'sides': 638,\n",
       " 'their': 639,\n",
       " 'wagons': 640,\n",
       " 'figures': 641,\n",
       " 'human': 642,\n",
       " 'bodies': 643,\n",
       " 'heads': 644,\n",
       " 'cats': 645,\n",
       " 'hawks': 646,\n",
       " 'rams': 647,\n",
       " 'lions': 648,\n",
       " 'therefore': 649,\n",
       " 'struggled': 650,\n",
       " 'endeavors': 651,\n",
       " 'call': 652,\n",
       " 'ill': 653,\n",
       " 'hovering': 654,\n",
       " 'plucks': 655,\n",
       " 'flowers': 656,\n",
       " 'weeds': 657,\n",
       " 'weaves': 658,\n",
       " 'chaplets': 659,\n",
       " 'them': 660,\n",
       " 'sails': 661,\n",
       " 'yellow': 662,\n",
       " 'leaves': 663,\n",
       " 'bits': 664,\n",
       " 'bark': 665,\n",
       " 'stream': 666,\n",
       " 'rejoicing': 667,\n",
       " 'safety': 668,\n",
       " 'weeping': 669,\n",
       " 'wreck': 670,\n",
       " 'whippoorwills': 671,\n",
       " 'fireflies': 672,\n",
       " 'act': 673,\n",
       " 'like': 674,\n",
       " 'creaters': 675,\n",
       " 'o': 676,\n",
       " 'gawd': 677,\n",
       " 'says': 678,\n",
       " 'hear': 679,\n",
       " 'things': 680,\n",
       " 'rushin': 681,\n",
       " 'talkin': 682,\n",
       " 'air': 683,\n",
       " 'daown': 684,\n",
       " 'thar': 685,\n",
       " 'ef': 686,\n",
       " 'stand': 687,\n",
       " 'right': 688,\n",
       " 'atween': 689,\n",
       " 'falls': 690,\n",
       " 'bear': 691,\n",
       " 'den': 692,\n",
       " 'many': 693,\n",
       " 'high': 694,\n",
       " 'belfast': 695,\n",
       " 'ensure': 696,\n",
       " 'shorter': 697,\n",
       " 'journeying': 698,\n",
       " 'south': 699,\n",
       " 'scotland': 700,\n",
       " 'joined': 701,\n",
       " 'poorer': 702,\n",
       " 'natives': 703,\n",
       " 'country': 704,\n",
       " 'poured': 705,\n",
       " 'consent': 706,\n",
       " 'answered': 707,\n",
       " 'gush': 708,\n",
       " 'tears': 709,\n",
       " 'telling': 710,\n",
       " 'persuaded': 711,\n",
       " 'goods': 712,\n",
       " 'cheap': 713,\n",
       " 'exchange': 714,\n",
       " 'guilt': 715,\n",
       " 'yet': 716,\n",
       " 'distant': 717,\n",
       " 'let': 718,\n",
       " 'simple': 719,\n",
       " 'justice': 720,\n",
       " 'acknowledge': 721,\n",
       " 'can': 722,\n",
       " 'recall': 723,\n",
       " 'suggestions': 724,\n",
       " 'rival': 725,\n",
       " 'side': 726,\n",
       " 'errors': 727,\n",
       " 'follies': 728,\n",
       " 'usual': 729,\n",
       " 'immature': 730,\n",
       " 'age': 731,\n",
       " 'seeming': 732,\n",
       " 'inexperience': 733,\n",
       " 'moral': 734,\n",
       " 'sense': 735,\n",
       " 'least': 736,\n",
       " 'general': 737,\n",
       " 'worldly': 738,\n",
       " 'wisdom': 739,\n",
       " 'keener': 740,\n",
       " 'thus': 741,\n",
       " 'happier': 742,\n",
       " 'less': 743,\n",
       " 'frequently': 744,\n",
       " 'rejected': 745,\n",
       " 'counsels': 746,\n",
       " 'embodied': 747,\n",
       " 'meaning': 748,\n",
       " 'whispers': 749,\n",
       " 'too': 750,\n",
       " 'cordially': 751,\n",
       " 'bitterly': 752,\n",
       " 'despised': 753,\n",
       " 'meeting': 754,\n",
       " 'betrothed': 755,\n",
       " 'avenue': 756,\n",
       " 'thronged': 757,\n",
       " 'lite': 758,\n",
       " 'city': 759,\n",
       " 'hastening': 760,\n",
       " 'greet': 761,\n",
       " 'best': 762,\n",
       " 'considered': 763,\n",
       " 'bows': 764,\n",
       " 'particle': 765,\n",
       " 'foreign': 766,\n",
       " 'matter': 767,\n",
       " 'lodging': 768,\n",
       " 'corner': 769,\n",
       " 'eye': 770,\n",
       " 'moment': 771,\n",
       " 'completely': 772,\n",
       " 'blind': 773,\n",
       " 'ever': 774,\n",
       " 'behold': 775,\n",
       " 'lovely': 776,\n",
       " 'woman': 777,\n",
       " 'beautiful': 778,\n",
       " 'doubt': 779,\n",
       " 'design': 780,\n",
       " 'angelo': 781,\n",
       " 'ricci': 782,\n",
       " 'joe': 783,\n",
       " 'czanek': 784,\n",
       " 'manuel': 785,\n",
       " 'silva': 786,\n",
       " 'terrible': 787,\n",
       " 'rushed': 788,\n",
       " 'towards': 789,\n",
       " 'embraced': 790,\n",
       " 'blessing': 791,\n",
       " 'god': 792,\n",
       " 'preservation': 793,\n",
       " 'fallen': 794,\n",
       " 'will': 795,\n",
       " 'scarcely': 796,\n",
       " 'denied': 797,\n",
       " 'we': 798,\n",
       " 'walked': 799,\n",
       " 'gardens': 800,\n",
       " 'evening': 801,\n",
       " 'retired': 802,\n",
       " 'asked': 803,\n",
       " 'stay': 804,\n",
       " 'read': 805,\n",
       " 'first': 806,\n",
       " 'here': 807,\n",
       " 'mother': 808,\n",
       " 'dante': 809,\n",
       " 'go': 810,\n",
       " 'off': 811,\n",
       " 'felix': 812,\n",
       " 'soon': 813,\n",
       " 'learned': 814,\n",
       " 'treacherous': 815,\n",
       " 'whom': 816,\n",
       " 'family': 817,\n",
       " 'endured': 818,\n",
       " 'such': 819,\n",
       " 'unheard': 820,\n",
       " 'oppression': 821,\n",
       " 'discovering': 822,\n",
       " 'deliverer': 823,\n",
       " 'reduced': 824,\n",
       " 'poverty': 825,\n",
       " 'ruin': 826,\n",
       " 'traitor': 827,\n",
       " 'good': 828,\n",
       " 'feeling': 829,\n",
       " 'honour': 830,\n",
       " 'quitted': 831,\n",
       " 'italy': 832,\n",
       " 'daughter': 833,\n",
       " 'insultingly': 834,\n",
       " 'sending': 835,\n",
       " 'pittance': 836,\n",
       " 'money': 837,\n",
       " 'aid': 838,\n",
       " 'plan': 839,\n",
       " 'future': 840,\n",
       " 'maintenance': 841,\n",
       " 'introduce': 842,\n",
       " 'gentleman': 843,\n",
       " 'am': 844,\n",
       " 'public': 845,\n",
       " 'well': 846,\n",
       " 'importance': 847,\n",
       " 'feel': 848,\n",
       " 'convinced': 849,\n",
       " 'name': 850,\n",
       " 'unaccountably': 851,\n",
       " 'forgotten': 852,\n",
       " 'everybody': 853,\n",
       " 'got': 854,\n",
       " 'aout': 855,\n",
       " 'idee': 856,\n",
       " 'dyin': 857,\n",
       " 'excep': 858,\n",
       " 'canoe': 859,\n",
       " 'wars': 860,\n",
       " 'islanders': 861,\n",
       " 'sacrifices': 862,\n",
       " 'sea': 863,\n",
       " 'gods': 864,\n",
       " 'below': 865,\n",
       " 'snake': 866,\n",
       " 'bite': 867,\n",
       " 'plague': 868,\n",
       " 'sharp': 869,\n",
       " 'gallopin': 870,\n",
       " 'ailments': 871,\n",
       " 'somethin': 872,\n",
       " 'afore': 873,\n",
       " 'cud': 874,\n",
       " 'take': 875,\n",
       " 'water': 876,\n",
       " 'simply': 877,\n",
       " 'looked': 878,\n",
       " 'forrad': 879,\n",
       " 'wa': 880,\n",
       " 'n': 881,\n",
       " 't': 882,\n",
       " 'horrible': 883,\n",
       " 'arter': 884,\n",
       " 'fields': 885,\n",
       " 'wide': 886,\n",
       " 'spread': 887,\n",
       " 'woods': 888,\n",
       " 'seem': 889,\n",
       " 'interminable': 890,\n",
       " 'exhausted': 891,\n",
       " 'harris': 892,\n",
       " 'information': 893,\n",
       " 'furnish': 894,\n",
       " 'attention': 895,\n",
       " 'early': 896,\n",
       " 'town': 897,\n",
       " 'records': 898,\n",
       " 'deeds': 899,\n",
       " 'zeal': 900,\n",
       " 'penetrating': 901,\n",
       " 'uncle': 902,\n",
       " 'occasionally': 903,\n",
       " 'shewn': 904,\n",
       " 'same': 905,\n",
       " 'seventh': 906,\n",
       " 'apartment': 907,\n",
       " 'closely': 908,\n",
       " 'shrouded': 909,\n",
       " 'velvet': 910,\n",
       " 'tapestries': 911,\n",
       " 'hung': 912,\n",
       " 'ceiling': 913,\n",
       " 'down': 914,\n",
       " 'walls': 915,\n",
       " 'falling': 916,\n",
       " 'heavy': 917,\n",
       " 'folds': 918,\n",
       " 'carpet': 919,\n",
       " 'material': 920,\n",
       " 'hue': 921,\n",
       " 'note': 922,\n",
       " 'implored': 923,\n",
       " 'mercy': 924,\n",
       " 'sake': 925,\n",
       " 'curiosity': 926,\n",
       " 'wait': 927,\n",
       " 'prepared': 928,\n",
       " 'account': 929,\n",
       " 'german': 930,\n",
       " 'marvels': 931,\n",
       " 'terrors': 932,\n",
       " 'beset': 933,\n",
       " 'bare': 934,\n",
       " 'boards': 935,\n",
       " 'probably': 936,\n",
       " 'known': 937,\n",
       " 'plaster': 938,\n",
       " 'whilst': 939,\n",
       " 'abundance': 940,\n",
       " 'dust': 941,\n",
       " 'cobwebs': 942,\n",
       " 'made': 943,\n",
       " 'inhabited': 944,\n",
       " 'partial': 945,\n",
       " 'shelter': 946,\n",
       " 'point': 947,\n",
       " 'rowley': 948,\n",
       " 'uncomfortably': 949,\n",
       " 'near': 950,\n",
       " 'according': 951,\n",
       " 'window': 952,\n",
       " 'view': 953,\n",
       " 'children': 954,\n",
       " 'easily': 955,\n",
       " 'distracted': 956,\n",
       " 'again': 957,\n",
       " 'prospect': 958,\n",
       " 'amusement': 959,\n",
       " 'muse': 960,\n",
       " 'long': 961,\n",
       " 'unwearied': 962,\n",
       " 'hours': 963,\n",
       " 'riveted': 964,\n",
       " 'frivolous': 965,\n",
       " 'device': 966,\n",
       " 'margin': 967,\n",
       " 'typography': 968,\n",
       " 'book': 969,\n",
       " 'absorbed': 970,\n",
       " 'summer': 971,\n",
       " 'quaint': 972,\n",
       " 'shadow': 973,\n",
       " 'aslant': 974,\n",
       " 'tapestry': 975,\n",
       " 'lose': 976,\n",
       " 'entire': 977,\n",
       " 'watching': 978,\n",
       " 'steady': 979,\n",
       " 'lamp': 980,\n",
       " 'embers': 981,\n",
       " 'away': 982,\n",
       " 'days': 983,\n",
       " 'perfume': 984,\n",
       " 'flower': 985,\n",
       " 'repeat': 986,\n",
       " 'monotonously': 987,\n",
       " 'common': 988,\n",
       " 'word': 989,\n",
       " 'until': 990,\n",
       " 'sound': 991,\n",
       " 'dint': 992,\n",
       " 'frequent': 993,\n",
       " 'repetition': 994,\n",
       " 'ceased': 995,\n",
       " 'convey': 996,\n",
       " 'idea': 997,\n",
       " 'whatever': 998,\n",
       " 'physical': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.27130000e-01,   1.34660000e-01,  -3.49210000e-01,\n",
       "         4.81920000e-01,  -4.93770000e-01,  -3.87940000e-02,\n",
       "        -1.36500000e+00,  -6.52720000e-02,  -2.83830000e-01,\n",
       "        -5.63370000e-01,  -2.60910000e-01,  -2.14390000e-02,\n",
       "        -1.68270000e-01,  -1.90030000e-01,   1.46610000e-01,\n",
       "         4.86780000e-01,   1.23060000e-01,  -3.40320000e-01,\n",
       "         1.81250000e-01,   5.68720000e-01,  -1.14460000e-03,\n",
       "        -5.96980000e-01,   1.30130000e-01,   7.11450000e-02,\n",
       "        -5.22910000e-02,   1.61650000e-01,  -2.21380000e-01,\n",
       "        -4.95220000e-01,   1.44850000e-01,  -1.23160000e-01,\n",
       "         9.43030000e-02,   2.70700000e-01,   3.47260000e-02,\n",
       "         4.33490000e-01,   8.71990000e-02,  -1.09690000e-01,\n",
       "        -3.34760000e-01,   2.29290000e-01,   1.56970000e-01,\n",
       "         1.58630000e-01,  -1.97850000e-01,   5.14440000e-02,\n",
       "        -4.40730000e-01,   2.08360000e-01,   7.55970000e-02,\n",
       "        -3.68940000e-01,  -1.75310000e-02,   5.62600000e-02,\n",
       "         3.93250000e-01,   5.95630000e-01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vector[\"confessed\"][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=word_to_ix['confessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.27130014e-01,   1.34660006e-01,  -3.49209994e-01,\n",
       "         4.81920004e-01,  -4.93770003e-01,  -3.87939997e-02,\n",
       "        -1.36500001e+00,  -6.52720034e-02,  -2.83829987e-01,\n",
       "        -5.63369989e-01,  -2.60910004e-01,  -2.14389991e-02,\n",
       "        -1.68270007e-01,  -1.90029994e-01,   1.46610007e-01,\n",
       "         4.86779988e-01,   1.23060003e-01,  -3.40319991e-01,\n",
       "         1.81250006e-01,   5.68719983e-01,  -1.14459998e-03,\n",
       "        -5.96979976e-01,   1.30129993e-01,   7.11449981e-02,\n",
       "        -5.22909984e-02,   1.61650002e-01,  -2.21379995e-01,\n",
       "        -4.95220006e-01,   1.44850001e-01,  -1.23159997e-01,\n",
       "         9.43029970e-02,   2.70700008e-01,   3.47260013e-02,\n",
       "         4.33490008e-01,   8.71990025e-02,  -1.09690003e-01,\n",
       "        -3.34760010e-01,   2.29289994e-01,   1.56969994e-01,\n",
       "         1.58629999e-01,  -1.97850004e-01,   5.14440015e-02,\n",
       "        -4.40730006e-01,   2.08360001e-01,   7.55970031e-02,\n",
       "        -3.68939996e-01,  -1.75310001e-02,   5.62600009e-02,\n",
       "         3.93249989e-01,   5.95629990e-01], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[idx][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2V_DIM = 300\n",
    "HIDDEN_DIM = 100\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GruClassifierW2vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, label_size, pre_trained_weights, dropout):\n",
    "        super(GruClassifierW2vec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data=torch.Tensor(pre_trained_weights)\n",
    "        self.gru = nn.GRU(input_size = embedding_dim,\n",
    "                            hidden_size = hidden_dim,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        return (Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        for i in range(self.num_layers):\n",
    "            gru_out, self.hidden = self.gru(x, self.hidden)\n",
    "        y  = self.hidden2label(gru_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = GruClassifierW2vec(embedding_dim=W2V_DIM,\n",
    "                            hidden_dim=HIDDEN_DIM,\n",
    "                            num_layers=NUM_LAYERS,\n",
    "                            vocab_size=VOCAB_SIZE,\n",
    "                            label_size=NUM_LABELS,\n",
    "                            pre_trained_weights = weights,\n",
    "                            dropout = DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GruClassifierW2vec (\n",
       "  (word_embeddings): Embedding(27369, 300)\n",
       "  (gru): GRU(300, 100, num_layers=3, dropout=0.7)\n",
       "  (hidden2label): Linear (100 -> 3)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EAP': 0, 'HPL': 2, 'MWS': 1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7900"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EAP_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5635"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPL_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6044"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MWS_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6526"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg=int((EAP_size+HPL_size+MWS_size)/3)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260759493670886\n",
      "1.1581188997338066\n",
      "1.0797485109199205\n"
     ]
    }
   ],
   "source": [
    "print(float(avg/EAP_size))\n",
    "print(float(avg/HPL_size))\n",
    "print(float(avg/MWS_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6525.4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EAP_size*0.826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6525.33"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HPL_size*1.158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6521.476"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MWS_size*1.079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EAP': 0, 'HPL': 2, 'MWS': 1}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loss function with mask to compensate class inbalance\n",
    "mask=torch.FloatTensor((0.826,1.158,1.079))\n",
    "#loss_function = nn.CrossEntropyLoss(weight=mask.view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "#print(loss_function.weight)\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he could he was sure get out by midnight though it is characteristic of him that this thought was untinged with eerie implications '"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=train_data[2][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 28\n",
       " 29\n",
       " 28\n",
       "  4\n",
       " 30\n",
       " 31\n",
       " 32\n",
       " 33\n",
       " 34\n",
       " 35\n",
       " 36\n",
       " 37\n",
       " 38\n",
       "  2\n",
       " 39\n",
       " 40\n",
       " 41\n",
       " 42\n",
       "  4\n",
       " 43\n",
       " 25\n",
       " 44\n",
       " 45\n",
       "[torch.cuda.LongTensor of size 23 (GPU 0)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_context=Variable(make_context_vector(sample,word_to_ix)).cuda()\n",
    "sample_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.0951 -1.2024 -1.0078\n",
       "[torch.cuda.FloatTensor of size 1x3 (GPU 0)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=model(sample_context)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n_iters = 2000\n",
    "num_epochs = n_iters/(len(x_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 500. Loss: 1.0518038272857666\n",
      "Iterations: 1000. Loss: 0.9179424047470093\n",
      "Iterations: 1500. Loss: 1.1740186214447021\n",
      "Iterations: 2000. Loss: 0.4193210005760193\n",
      "Iterations: 2500. Loss: 0.9742138981819153\n",
      "Iterations: 3000. Loss: 0.9318205118179321\n",
      "Iterations: 3500. Loss: 0.4751545190811157\n",
      "Iterations: 4000. Loss: 0.18728940188884735\n",
      "Iterations: 4500. Loss: 1.9101083278656006\n",
      "Iterations: 5000. Loss: 0.9721226096153259\n",
      "Iterations: 5500. Loss: 0.9946901202201843\n",
      "Iterations: 6000. Loss: 0.5614168643951416\n",
      "Iterations: 6500. Loss: 2.71254825592041\n",
      "Iterations: 7000. Loss: 2.937105178833008\n",
      "Iterations: 7500. Loss: 2.4545650482177734\n",
      "Iterations: 8000. Loss: 1.4660981893539429\n",
      "Iterations: 8500. Loss: 1.6912579536437988\n",
      "Iterations: 9000. Loss: 0.21140481531620026\n",
      "Iterations: 9500. Loss: 0.3685607612133026\n",
      "Iterations: 10000. Loss: 0.9854797124862671\n",
      "Iterations: 10500. Loss: 0.5488322973251343\n",
      "Iterations: 11000. Loss: 2.1286096572875977\n",
      "Iterations: 11500. Loss: 2.3581249713897705\n",
      "Iterations: 12000. Loss: 0.22693882882595062\n",
      "Iterations: 12500. Loss: 2.3922009468078613\n",
      "Iterations: 13000. Loss: 1.080806016921997\n",
      "Iterations: 13500. Loss: 0.12795650959014893\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for (sent,label) in train_data:\n",
    "        # Step 1 - clear the gradients\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "    \n",
    "        ## Avoid breaking for empty input\n",
    "        try:\n",
    "            ## Step 2- Prepare input and label\n",
    "            context_vec = Variable(make_context_vector(sent, word_to_ix)).cuda()\n",
    "            target = Variable(make_target(label, label_to_ix)).cuda()\n",
    "            # Step 3 - Run forward pass\n",
    "            output = model(context_vec)  \n",
    "            # Step 4 - Compute loss, gradients, update parameters\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except:\n",
    "            pass\n",
    "        iter+=1      \n",
    "        ## Calculate final accuracy\n",
    "        if iter % 500 ==0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (sent,label) in valid_data:\n",
    "                context_vec = Variable(make_context_vector(sent, word_to_ix)).cuda()\n",
    "                target = Variable(make_target(label, label_to_ix)).cuda()\n",
    "                output = model(context_vec)\n",
    "                _,predicted = torch.max(output.data,1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted[0] == make_target(label, label_to_ix)).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print('Iterations: {}. Loss: {}'.format(iter,loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Making predictions on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "bow_vec = Variable(make_context_vector(valid_data[n][0], word_to_ix))\n",
    "print(\"-\"*20 + \" INPUT \"+\"-\"*30)\n",
    "print(\"TRUE LABEL = {}\".format(valid_data[n][1]))\n",
    "print(\"SENTENCE = {}\".format(valid_data[n][0]))\n",
    "print(\"-\"*20 + \" PREDICTION \"+\"-\"*30)\n",
    "log_probs = model(bow_vec)\n",
    "_,predicted = torch.max(log_probs.data,1)\n",
    "print(\"PRED = {}\".format(predicted[0]))\n",
    "print(\"PRED = {}\".format(list(label_to_ix.keys())[list(label_to_ix.values()).index(predicted[0])]))\n",
    "##print(\"LOG_PROB = {}\".format(log_probs))\n",
    "print(\"PROBS = {}\".format(F.softmax(log_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(model,test):\n",
    "    my_sub = pd.DataFrame(columns={'id', 'EAP','HPL', 'MWS'})\n",
    "    my_sub=my_sub[['id', 'EAP','HPL', 'MWS']]\n",
    "    for i in range(len(test['phrase_preprocessed'])):\n",
    "        sample=test['phrase_preprocessed'][i]\n",
    "        #print(sample)\n",
    "        sample_context=Variable(make_context_vector(sample,word_to_ix)).cuda()\n",
    "        log_prob=model(sample_context)\n",
    "        probs=F.softmax(log_prob)\n",
    "        my_sub.loc[i] = [test['id'][i], probs.data[0][0],probs.data[0][1],probs.data[0][2]]\n",
    "    return my_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=make_preds(model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds.to_csv('roberto_new_11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
